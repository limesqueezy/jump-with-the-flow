{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchsde\n",
    "from torchdyn.core import NeuralODE\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToPILImage\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchcfm.conditional_flow_matching import *\n",
    "from torchcfm.models.unet import UNetModel\n",
    "\n",
    "\n",
    "\n",
    "savedir = \"models/mnist\"\n",
    "os.makedirs(savedir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchmetrics in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy>1.20.0 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torchmetrics) (2.2.5)\n",
      "Requirement already satisfied: packaging>17.1 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torchmetrics) (24.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torchmetrics) (2.7.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torchmetrics) (0.14.3)\n",
      "Requirement already satisfied: setuptools in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (78.1.1)\n",
      "Requirement already satisfied: typing_extensions in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.13.2)\n",
      "Requirement already satisfied: filelock in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics) (3.18.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics) (3.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"From https://raw.githubusercontent.com/openai/guided-diffusion/main/guided_diffusion/unet.py.\"\"\"\n",
    "import math\n",
    "from abc import abstractmethod\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"Helpers to train with 16-bit precision.\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from torch._utils import _flatten_dense_tensors, _unflatten_dense_tensors\n",
    "\n",
    "\"\"\"Logger copied from OpenAI baselines to avoid extra RL-based dependencies:\n",
    "\n",
    "https://github.com/openai/baselines/blob/ea25b9e8b234e6ee1bca43083f8f3cf974143998/baselines/logger.py\n",
    "\"\"\"\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import os.path as osp\n",
    "import shutil\n",
    "import sys\n",
    "import tempfile\n",
    "import time\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from contextlib import contextmanager\n",
    "\n",
    "DEBUG = 10\n",
    "INFO = 20\n",
    "WARN = 30\n",
    "ERROR = 40\n",
    "\n",
    "DISABLED = 50\n",
    "\n",
    "\"\"\"Various utilities for neural networks.\"\"\"\n",
    "\n",
    "import math\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# PyTorch 1.7 has SiLU, but we support PyTorch 1.5.\n",
    "class SiLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * th.sigmoid(x)\n",
    "\n",
    "\n",
    "class GroupNorm32(nn.GroupNorm):\n",
    "    def forward(self, x):\n",
    "        return super().forward(x.float()).type(x.dtype)\n",
    "\n",
    "\n",
    "def conv_nd(dims, *args, **kwargs):\n",
    "    \"\"\"Create a 1D, 2D, or 3D convolution module.\"\"\"\n",
    "    if dims == 1:\n",
    "        return nn.Conv1d(*args, **kwargs)\n",
    "    elif dims == 2:\n",
    "        return nn.Conv2d(*args, **kwargs)\n",
    "    elif dims == 3:\n",
    "        return nn.Conv3d(*args, **kwargs)\n",
    "    raise ValueError(f\"unsupported dimensions: {dims}\")\n",
    "\n",
    "\n",
    "def linear(*args, **kwargs):\n",
    "    \"\"\"Create a linear module.\"\"\"\n",
    "    return nn.Linear(*args, **kwargs)\n",
    "\n",
    "\n",
    "def avg_pool_nd(dims, *args, **kwargs):\n",
    "    \"\"\"Create a 1D, 2D, or 3D average pooling module.\"\"\"\n",
    "    if dims == 1:\n",
    "        return nn.AvgPool1d(*args, **kwargs)\n",
    "    elif dims == 2:\n",
    "        return nn.AvgPool2d(*args, **kwargs)\n",
    "    elif dims == 3:\n",
    "        return nn.AvgPool3d(*args, **kwargs)\n",
    "    raise ValueError(f\"unsupported dimensions: {dims}\")\n",
    "\n",
    "\n",
    "def update_ema(target_params, source_params, rate=0.99):\n",
    "    \"\"\"Update target parameters to be closer to those of source parameters using an exponential\n",
    "    moving average.\n",
    "\n",
    "    :param target_params: the target parameter sequence.\n",
    "    :param source_params: the source parameter sequence.\n",
    "    :param rate: the EMA rate (closer to 1 means slower).\n",
    "    \"\"\"\n",
    "    for targ, src in zip(target_params, source_params):\n",
    "        targ.detach().mul_(rate).add_(src, alpha=1 - rate)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def zero_module(module, std=1e-3):\n",
    "    \"\"\"Initialize the parameters of a module with small Gaussian noise and return it.\"\"\"\n",
    "    for p in module.parameters():\n",
    "        if p.dim() > 1:  # typically weights\n",
    "            nn.init.normal_(p, mean=0.0, std=std)\n",
    "        else:  # typically biases\n",
    "            nn.init.zeros_(p)\n",
    "    return module\n",
    "\n",
    "\n",
    "def scale_module(module, scale):\n",
    "    \"\"\"Scale the parameters of a module and return it.\"\"\"\n",
    "    for p in module.parameters():\n",
    "        p.detach().mul_(scale)\n",
    "    return module\n",
    "\n",
    "\n",
    "def mean_flat(tensor):\n",
    "    \"\"\"Take the mean over all non-batch dimensions.\"\"\"\n",
    "    return tensor.mean(dim=list(range(1, len(tensor.shape))))\n",
    "\n",
    "\n",
    "def normalization(channels):\n",
    "    \"\"\"Make a standard normalization layer.\n",
    "\n",
    "    :param channels: number of input channels.\n",
    "    :return: an nn.Module for normalization.\n",
    "    \"\"\"\n",
    "    return GroupNorm32(32, channels)\n",
    "\n",
    "\n",
    "def timestep_embedding(timesteps, dim, max_period=10000):\n",
    "    \"\"\"Create sinusoidal timestep embeddings.\n",
    "\n",
    "    :param timesteps: a 1-D Tensor of N indices, one per batch element. These may be fractional.\n",
    "    :param dim: the dimension of the output.\n",
    "    :param max_period: controls the minimum frequency of the embeddings.\n",
    "    :return: an [N x dim] Tensor of positional embeddings.\n",
    "    \"\"\"\n",
    "    half = dim // 2\n",
    "    freqs = th.exp(\n",
    "        -math.log(max_period)\n",
    "        * th.arange(start=0, end=half, dtype=th.float32, device=timesteps.device)\n",
    "        / half\n",
    "    )\n",
    "    args = timesteps[:, None].float() * freqs[None]\n",
    "    embedding = th.cat([th.cos(args), th.sin(args)], dim=-1)\n",
    "    if dim % 2:\n",
    "        embedding = th.cat([embedding, th.zeros_like(embedding[:, :1])], dim=-1)\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def checkpoint(func, inputs, params, flag):\n",
    "    \"\"\"Evaluate a function without caching intermediate activations, allowing for reduced memory at\n",
    "    the expense of extra compute in the backward pass.\n",
    "\n",
    "    :param func: the function to evaluate.\n",
    "    :param inputs: the argument sequence to pass to `func`.\n",
    "    :param params: a sequence of parameters `func` depends on but does not\n",
    "                   explicitly take as arguments.\n",
    "    :param flag: if False, disable gradient checkpointing.\n",
    "    \"\"\"\n",
    "    if flag:\n",
    "        args = tuple(inputs) + tuple(params)\n",
    "        return CheckpointFunction.apply(func, len(inputs), *args)\n",
    "    else:\n",
    "        return func(*inputs)\n",
    "\n",
    "\n",
    "class CheckpointFunction(th.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, run_function, length, *args):\n",
    "        ctx.run_function = run_function\n",
    "        ctx.input_tensors = list(args[:length])\n",
    "        ctx.input_params = list(args[length:])\n",
    "        with th.no_grad():\n",
    "            output_tensors = ctx.run_function(*ctx.input_tensors)\n",
    "        return output_tensors\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, *output_grads):\n",
    "        ctx.input_tensors = [x.detach().requires_grad_(True) for x in ctx.input_tensors]\n",
    "        with th.enable_grad():\n",
    "            # Fixes a bug where the first op in run_function modifies the\n",
    "            # Tensor storage in place, which is not allowed for detach()'d\n",
    "            # Tensors.\n",
    "            shallow_copies = [x.view_as(x) for x in ctx.input_tensors]\n",
    "            output_tensors = ctx.run_function(*shallow_copies)\n",
    "        input_grads = th.autograd.grad(\n",
    "            output_tensors,\n",
    "            ctx.input_tensors + ctx.input_params,\n",
    "            output_grads,\n",
    "            allow_unused=True,\n",
    "        )\n",
    "        del ctx.input_tensors\n",
    "        del ctx.input_params\n",
    "        del output_tensors\n",
    "        return (None, None) + input_grads\n",
    "\n",
    "\n",
    "\n",
    "class KVWriter:\n",
    "    def writekvs(self, kvs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class SeqWriter:\n",
    "    def writeseq(self, seq):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class HumanOutputFormat(KVWriter, SeqWriter):\n",
    "    def __init__(self, filename_or_file):\n",
    "        if isinstance(filename_or_file, str):\n",
    "            self.file = open(filename_or_file, \"w\")\n",
    "            self.own_file = True\n",
    "        else:\n",
    "            assert hasattr(filename_or_file, \"read\"), (\n",
    "                \"expected file or str, got %s\" % filename_or_file\n",
    "            )\n",
    "            self.file = filename_or_file\n",
    "            self.own_file = False\n",
    "\n",
    "    def writekvs(self, kvs):\n",
    "        # Create strings for printing\n",
    "        key2str = {}\n",
    "        for key, val in sorted(kvs.items()):\n",
    "            if hasattr(val, \"__float__\"):\n",
    "                valstr = \"%-8.3g\" % val\n",
    "            else:\n",
    "                valstr = str(val)\n",
    "            key2str[self._truncate(key)] = self._truncate(valstr)\n",
    "\n",
    "        # Find max widths\n",
    "        if len(key2str) == 0:\n",
    "            print(\"WARNING: tried to write empty key-value dict\")\n",
    "            return\n",
    "        else:\n",
    "            keywidth = max(map(len, key2str.keys()))\n",
    "            valwidth = max(map(len, key2str.values()))\n",
    "\n",
    "        # Write out the data\n",
    "        dashes = \"-\" * (keywidth + valwidth + 7)\n",
    "        lines = [dashes]\n",
    "        for key, val in sorted(key2str.items(), key=lambda kv: kv[0].lower()):\n",
    "            lines.append(\n",
    "                \"| %s%s | %s%s |\"\n",
    "                % (key, \" \" * (keywidth - len(key)), val, \" \" * (valwidth - len(val)))\n",
    "            )\n",
    "        lines.append(dashes)\n",
    "        self.file.write(\"\\n\".join(lines) + \"\\n\")\n",
    "\n",
    "        # Flush the output to the file\n",
    "        self.file.flush()\n",
    "\n",
    "    def _truncate(self, s):\n",
    "        maxlen = 30\n",
    "        return s[: maxlen - 3] + \"...\" if len(s) > maxlen else s\n",
    "\n",
    "    def writeseq(self, seq):\n",
    "        seq = list(seq)\n",
    "        for i, elem in enumerate(seq):\n",
    "            self.file.write(elem)\n",
    "            if i < len(seq) - 1:  # add space unless this is the last one\n",
    "                self.file.write(\" \")\n",
    "        self.file.write(\"\\n\")\n",
    "        self.file.flush()\n",
    "\n",
    "    def close(self):\n",
    "        if self.own_file:\n",
    "            self.file.close()\n",
    "\n",
    "\n",
    "class JSONOutputFormat(KVWriter):\n",
    "    def __init__(self, filename):\n",
    "        self.file = open(filename, \"w\")\n",
    "\n",
    "    def writekvs(self, kvs):\n",
    "        for k, v in sorted(kvs.items()):\n",
    "            if hasattr(v, \"dtype\"):\n",
    "                kvs[k] = float(v)\n",
    "        self.file.write(json.dumps(kvs) + \"\\n\")\n",
    "        self.file.flush()\n",
    "\n",
    "    def close(self):\n",
    "        self.file.close()\n",
    "\n",
    "\n",
    "class CSVOutputFormat(KVWriter):\n",
    "    def __init__(self, filename):\n",
    "        self.file = open(filename, \"w+t\")\n",
    "        self.keys = []\n",
    "        self.sep = \",\"\n",
    "\n",
    "    def writekvs(self, kvs):\n",
    "        # Add our current row to the history\n",
    "        extra_keys = list(kvs.keys() - self.keys)\n",
    "        extra_keys.sort()\n",
    "        if extra_keys:\n",
    "            self.keys.extend(extra_keys)\n",
    "            self.file.seek(0)\n",
    "            lines = self.file.readlines()\n",
    "            self.file.seek(0)\n",
    "            for i, k in enumerate(self.keys):\n",
    "                if i > 0:\n",
    "                    self.file.write(\",\")\n",
    "                self.file.write(k)\n",
    "            self.file.write(\"\\n\")\n",
    "            for line in lines[1:]:\n",
    "                self.file.write(line[:-1])\n",
    "                self.file.write(self.sep * len(extra_keys))\n",
    "                self.file.write(\"\\n\")\n",
    "        for i, k in enumerate(self.keys):\n",
    "            if i > 0:\n",
    "                self.file.write(\",\")\n",
    "            v = kvs.get(k)\n",
    "            if v is not None:\n",
    "                self.file.write(str(v))\n",
    "        self.file.write(\"\\n\")\n",
    "        self.file.flush()\n",
    "\n",
    "    def close(self):\n",
    "        self.file.close()\n",
    "\n",
    "\n",
    "class TensorBoardOutputFormat(KVWriter):\n",
    "    \"\"\"Dumps key/value pairs into TensorBoard's numeric format.\"\"\"\n",
    "\n",
    "    def __init__(self, dir):\n",
    "        os.makedirs(dir, exist_ok=True)\n",
    "        self.dir = dir\n",
    "        self.step = 1\n",
    "        prefix = \"events\"\n",
    "        path = osp.join(osp.abspath(dir), prefix)\n",
    "        import tensorflow as tf\n",
    "        from tensorflow.core.util import event_pb2\n",
    "        from tensorflow.python import pywrap_tensorflow\n",
    "        from tensorflow.python.util import compat\n",
    "\n",
    "        self.tf = tf\n",
    "        self.event_pb2 = event_pb2\n",
    "        self.pywrap_tensorflow = pywrap_tensorflow\n",
    "        self.writer = pywrap_tensorflow.EventsWriter(compat.as_bytes(path))\n",
    "\n",
    "    def writekvs(self, kvs):\n",
    "        def summary_val(k, v):\n",
    "            kwargs = {\"tag\": k, \"simple_value\": float(v)}\n",
    "            return self.tf.Summary.Value(**kwargs)\n",
    "\n",
    "        summary = self.tf.Summary(value=[summary_val(k, v) for k, v in kvs.items()])\n",
    "        event = self.event_pb2.Event(wall_time=time.time(), summary=summary)\n",
    "        event.step = self.step  # is there any reason why you'd want to specify the step?\n",
    "        self.writer.WriteEvent(event)\n",
    "        self.writer.Flush()\n",
    "        self.step += 1\n",
    "\n",
    "    def close(self):\n",
    "        if self.writer:\n",
    "            self.writer.Close()\n",
    "            self.writer = None\n",
    "\n",
    "\n",
    "def make_output_format(format, ev_dir, log_suffix=\"\"):\n",
    "    os.makedirs(ev_dir, exist_ok=True)\n",
    "    if format == \"stdout\":\n",
    "        return HumanOutputFormat(sys.stdout)\n",
    "    elif format == \"log\":\n",
    "        return HumanOutputFormat(osp.join(ev_dir, \"log%s.txt\" % log_suffix))\n",
    "    elif format == \"json\":\n",
    "        return JSONOutputFormat(osp.join(ev_dir, \"progress%s.json\" % log_suffix))\n",
    "    elif format == \"csv\":\n",
    "        return CSVOutputFormat(osp.join(ev_dir, \"progress%s.csv\" % log_suffix))\n",
    "    elif format == \"tensorboard\":\n",
    "        return TensorBoardOutputFormat(osp.join(ev_dir, \"tb%s\" % log_suffix))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown format specified: {format}\")\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# API\n",
    "# ================================================================\n",
    "\n",
    "\n",
    "def logkv(key, val):\n",
    "    \"\"\"Log a value of some diagnostic Call this once for each diagnostic quantity, each iteration\n",
    "    If called many times, last value will be used.\"\"\"\n",
    "    get_current().logkv(key, val)\n",
    "\n",
    "\n",
    "def logkv_mean(key, val):\n",
    "    \"\"\"The same as logkv(), but if called many times, values averaged.\"\"\"\n",
    "    get_current().logkv_mean(key, val)\n",
    "\n",
    "\n",
    "def logkvs(d):\n",
    "    \"\"\"Log a dictionary of key-value pairs.\"\"\"\n",
    "    for k, v in d.items():\n",
    "        logkv(k, v)\n",
    "\n",
    "\n",
    "def dumpkvs():\n",
    "    \"\"\"Write all of the diagnostics from the current iteration.\"\"\"\n",
    "    return get_current().dumpkvs()\n",
    "\n",
    "\n",
    "def getkvs():\n",
    "    return get_current().name2val\n",
    "\n",
    "\n",
    "def log(*args, level=INFO):\n",
    "    \"\"\"Write the sequence of args, with no separators, to the console and output files (if you've\n",
    "    configured an output file).\"\"\"\n",
    "    get_current().log(*args, level=level)\n",
    "\n",
    "\n",
    "def debug(*args):\n",
    "    log(*args, level=DEBUG)\n",
    "\n",
    "\n",
    "def info(*args):\n",
    "    log(*args, level=INFO)\n",
    "\n",
    "\n",
    "def warn(*args):\n",
    "    log(*args, level=WARN)\n",
    "\n",
    "\n",
    "def error(*args):\n",
    "    log(*args, level=ERROR)\n",
    "\n",
    "\n",
    "def set_level(level):\n",
    "    \"\"\"Set logging threshold on current logger.\"\"\"\n",
    "    get_current().set_level(level)\n",
    "\n",
    "\n",
    "def set_comm(comm):\n",
    "    get_current().set_comm(comm)\n",
    "\n",
    "\n",
    "def get_dir():\n",
    "    \"\"\"Get directory that log files are being written to.\n",
    "\n",
    "    will be None if there is no output directory (i.e., if you didn't call start)\n",
    "    \"\"\"\n",
    "    return get_current().get_dir()\n",
    "\n",
    "\n",
    "record_tabular = logkv\n",
    "dump_tabular = dumpkvs\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def profile_kv(scopename):\n",
    "    logkey = \"wait_\" + scopename\n",
    "    tstart = time.time()\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        get_current().name2val[logkey] += time.time() - tstart\n",
    "\n",
    "\n",
    "def profile(n):\n",
    "    \"\"\"\n",
    "    Usage:\n",
    "    @profile(\"my_func\")\n",
    "    def my_func(): code\n",
    "    \"\"\"\n",
    "\n",
    "    def decorator_with_name(func):\n",
    "        def func_wrapper(*args, **kwargs):\n",
    "            with profile_kv(n):\n",
    "                return func(*args, **kwargs)\n",
    "\n",
    "        return func_wrapper\n",
    "\n",
    "    return decorator_with_name\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# Backend\n",
    "# ================================================================\n",
    "\n",
    "\n",
    "def get_current():\n",
    "    if Logger.CURRENT is None:\n",
    "        _configure_default_logger()\n",
    "\n",
    "    return Logger.CURRENT\n",
    "\n",
    "\n",
    "class Logger:\n",
    "    DEFAULT = None  # A logger with no output files. (See right below class definition)\n",
    "    # So that you can still log to the terminal without setting up any output files\n",
    "    CURRENT = None  # Current logger being used by the free functions above\n",
    "\n",
    "    def __init__(self, dir, output_formats, comm=None):\n",
    "        self.name2val = defaultdict(float)  # values this iteration\n",
    "        self.name2cnt = defaultdict(int)\n",
    "        self.level = INFO\n",
    "        self.dir = dir\n",
    "        self.output_formats = output_formats\n",
    "        self.comm = comm\n",
    "\n",
    "    # Logging API, forwarded\n",
    "    # ----------------------------------------\n",
    "    def logkv(self, key, val):\n",
    "        self.name2val[key] = val\n",
    "\n",
    "    def logkv_mean(self, key, val):\n",
    "        oldval, cnt = self.name2val[key], self.name2cnt[key]\n",
    "        self.name2val[key] = oldval * cnt / (cnt + 1) + val / (cnt + 1)\n",
    "        self.name2cnt[key] = cnt + 1\n",
    "\n",
    "    def dumpkvs(self):\n",
    "        if self.comm is None:\n",
    "            d = self.name2val\n",
    "        else:\n",
    "            d = mpi_weighted_mean(\n",
    "                self.comm,\n",
    "                {name: (val, self.name2cnt.get(name, 1)) for (name, val) in self.name2val.items()},\n",
    "            )\n",
    "            if self.comm.rank != 0:\n",
    "                d[\"dummy\"] = 1  # so we don't get a warning about empty dict\n",
    "        out = d.copy()  # Return the dict for unit testing purposes\n",
    "        for fmt in self.output_formats:\n",
    "            if isinstance(fmt, KVWriter):\n",
    "                fmt.writekvs(d)\n",
    "        self.name2val.clear()\n",
    "        self.name2cnt.clear()\n",
    "        return out\n",
    "\n",
    "    def log(self, *args, level=INFO):\n",
    "        if self.level <= level:\n",
    "            self._do_log(args)\n",
    "\n",
    "    # Configuration\n",
    "    # ----------------------------------------\n",
    "    def set_level(self, level):\n",
    "        self.level = level\n",
    "\n",
    "    def set_comm(self, comm):\n",
    "        self.comm = comm\n",
    "\n",
    "    def get_dir(self):\n",
    "        return self.dir\n",
    "\n",
    "    def close(self):\n",
    "        for fmt in self.output_formats:\n",
    "            fmt.close()\n",
    "\n",
    "    # Misc\n",
    "    # ----------------------------------------\n",
    "    def _do_log(self, args):\n",
    "        for fmt in self.output_formats:\n",
    "            if isinstance(fmt, SeqWriter):\n",
    "                fmt.writeseq(map(str, args))\n",
    "\n",
    "\n",
    "def get_rank_without_mpi_import():\n",
    "    # check environment variables here instead of importing mpi4py\n",
    "    # to avoid calling MPI_Init() when this module is imported\n",
    "    for varname in [\"PMI_RANK\", \"OMPI_COMM_WORLD_RANK\"]:\n",
    "        if varname in os.environ:\n",
    "            return int(os.environ[varname])\n",
    "    return 0\n",
    "\n",
    "\n",
    "def mpi_weighted_mean(comm, local_name2valcount):\n",
    "    \"\"\"\n",
    "    Copied from: https://github.com/openai/baselines/blob/ea25b9e8b234e6ee1bca43083f8f3cf974143998/baselines/common/mpi_util.py#L110\n",
    "    Perform a weighted average over dicts that are each on a different node\n",
    "    Input: local_name2valcount: dict mapping key -> (value, count)\n",
    "    Returns: key -> mean\n",
    "    \"\"\"\n",
    "    all_name2valcount = comm.gather(local_name2valcount)\n",
    "    if comm.rank == 0:\n",
    "        name2sum = defaultdict(float)\n",
    "        name2count = defaultdict(float)\n",
    "        for n2vc in all_name2valcount:\n",
    "            for name, (val, count) in n2vc.items():\n",
    "                try:\n",
    "                    val = float(val)\n",
    "                except ValueError:\n",
    "                    if comm.rank == 0:\n",
    "                        warnings.warn(f\"WARNING: tried to compute mean on non-float {name}={val}\")\n",
    "                else:\n",
    "                    name2sum[name] += val * count\n",
    "                    name2count[name] += count\n",
    "        return {name: name2sum[name] / name2count[name] for name in name2sum}\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "\n",
    "def configure(dir=None, format_strs=None, comm=None, log_suffix=\"\"):\n",
    "    \"\"\"If comm is provided, average all numerical stats across that comm.\"\"\"\n",
    "    if dir is None:\n",
    "        dir = os.getenv(\"OPENAI_LOGDIR\")\n",
    "    if dir is None:\n",
    "        dir = osp.join(\n",
    "            tempfile.gettempdir(),\n",
    "            datetime.datetime.now().strftime(\"openai-%Y-%m-%d-%H-%M-%S-%f\"),\n",
    "        )\n",
    "    assert isinstance(dir, str)\n",
    "    dir = os.path.expanduser(dir)\n",
    "    os.makedirs(os.path.expanduser(dir), exist_ok=True)\n",
    "\n",
    "    rank = get_rank_without_mpi_import()\n",
    "    if rank > 0:\n",
    "        log_suffix = log_suffix + \"-rank%03i\" % rank\n",
    "\n",
    "    if format_strs is None:\n",
    "        if rank == 0:\n",
    "            format_strs = os.getenv(\"OPENAI_LOG_FORMAT\", \"stdout,log,csv\").split(\",\")\n",
    "        else:\n",
    "            format_strs = os.getenv(\"OPENAI_LOG_FORMAT_MPI\", \"log\").split(\",\")\n",
    "    format_strs = filter(None, format_strs)\n",
    "    output_formats = [make_output_format(f, dir, log_suffix) for f in format_strs]\n",
    "\n",
    "    Logger.CURRENT = Logger(dir=dir, output_formats=output_formats, comm=comm)\n",
    "    if output_formats:\n",
    "        log(\"Logging to %s\" % dir)\n",
    "\n",
    "\n",
    "def _configure_default_logger():\n",
    "    configure()\n",
    "    Logger.DEFAULT = Logger.CURRENT\n",
    "\n",
    "\n",
    "def reset():\n",
    "    if Logger.CURRENT is not Logger.DEFAULT:\n",
    "        Logger.CURRENT.close()\n",
    "        Logger.CURRENT = Logger.DEFAULT\n",
    "        log(\"Reset logger\")\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def scoped_configure(dir=None, format_strs=None, comm=None):\n",
    "    prevlogger = Logger.CURRENT\n",
    "    configure(dir=dir, format_strs=format_strs, comm=comm)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        Logger.CURRENT.close()\n",
    "        Logger.CURRENT = prevlogger\n",
    "\n",
    "\n",
    "INITIAL_LOG_LOSS_SCALE = 20.0\n",
    "\n",
    "\n",
    "def convert_module_to_f16(l):\n",
    "    \"\"\"Convert primitive modules to float16.\"\"\"\n",
    "    if isinstance(l, (nn.Conv1d, nn.Conv2d, nn.Conv3d)):\n",
    "        l.weight.data = l.weight.data.half()\n",
    "        if l.bias is not None:\n",
    "            l.bias.data = l.bias.data.half()\n",
    "\n",
    "\n",
    "def convert_module_to_f32(l):\n",
    "    \"\"\"Convert primitive modules to float32, undoing convert_module_to_f16().\"\"\"\n",
    "    if isinstance(l, (nn.Conv1d, nn.Conv2d, nn.Conv3d)):\n",
    "        l.weight.data = l.weight.data.float()\n",
    "        if l.bias is not None:\n",
    "            l.bias.data = l.bias.data.float()\n",
    "\n",
    "\n",
    "def make_master_params(param_groups_and_shapes):\n",
    "    \"\"\"Copy model parameters into a (differently-shaped) list of full-precision parameters.\"\"\"\n",
    "    master_params = []\n",
    "    for param_group, shape in param_groups_and_shapes:\n",
    "        master_param = nn.Parameter(\n",
    "            _flatten_dense_tensors([param.detach().float() for (_, param) in param_group]).view(\n",
    "                shape\n",
    "            )\n",
    "        )\n",
    "        master_param.requires_grad = True\n",
    "        master_params.append(master_param)\n",
    "    return master_params\n",
    "\n",
    "\n",
    "def model_grads_to_master_grads(param_groups_and_shapes, master_params):\n",
    "    \"\"\"Copy the gradients from the model parameters into the master parameters from\n",
    "    make_master_params().\"\"\"\n",
    "    for master_param, (param_group, shape) in zip(master_params, param_groups_and_shapes):\n",
    "        master_param.grad = _flatten_dense_tensors(\n",
    "            [param_grad_or_zeros(param) for (_, param) in param_group]\n",
    "        ).view(shape)\n",
    "\n",
    "\n",
    "def master_params_to_model_params(param_groups_and_shapes, master_params):\n",
    "    \"\"\"Copy the master parameter data back into the model parameters.\"\"\"\n",
    "    # Without copying to a list, if a generator is passed, this will\n",
    "    # silently not copy any parameters.\n",
    "    for master_param, (param_group, _) in zip(master_params, param_groups_and_shapes):\n",
    "        for (_, param), unflat_master_param in zip(\n",
    "            param_group, unflatten_master_params(param_group, master_param.view(-1))\n",
    "        ):\n",
    "            param.detach().copy_(unflat_master_param)\n",
    "\n",
    "\n",
    "def unflatten_master_params(param_group, master_param):\n",
    "    return _unflatten_dense_tensors(master_param, [param for (_, param) in param_group])\n",
    "\n",
    "\n",
    "def get_param_groups_and_shapes(named_model_params):\n",
    "    named_model_params = list(named_model_params)\n",
    "    scalar_vector_named_params = (\n",
    "        [(n, p) for (n, p) in named_model_params if p.ndim <= 1],\n",
    "        (-1),\n",
    "    )\n",
    "    matrix_named_params = (\n",
    "        [(n, p) for (n, p) in named_model_params if p.ndim > 1],\n",
    "        (1, -1),\n",
    "    )\n",
    "    return [scalar_vector_named_params, matrix_named_params]\n",
    "\n",
    "\n",
    "def master_params_to_state_dict(model, param_groups_and_shapes, master_params, use_fp16):\n",
    "    if use_fp16:\n",
    "        state_dict = model.state_dict()\n",
    "        for master_param, (param_group, _) in zip(master_params, param_groups_and_shapes):\n",
    "            for (name, _), unflat_master_param in zip(\n",
    "                param_group, unflatten_master_params(param_group, master_param.view(-1))\n",
    "            ):\n",
    "                assert name in state_dict\n",
    "                state_dict[name] = unflat_master_param\n",
    "    else:\n",
    "        state_dict = model.state_dict()\n",
    "        for i, (name, _value) in enumerate(model.named_parameters()):\n",
    "            assert name in state_dict\n",
    "            state_dict[name] = master_params[i]\n",
    "    return state_dict\n",
    "\n",
    "\n",
    "def state_dict_to_master_params(model, state_dict, use_fp16):\n",
    "    if use_fp16:\n",
    "        named_model_params = [(name, state_dict[name]) for name, _ in model.named_parameters()]\n",
    "        param_groups_and_shapes = get_param_groups_and_shapes(named_model_params)\n",
    "        master_params = make_master_params(param_groups_and_shapes)\n",
    "    else:\n",
    "        master_params = [state_dict[name] for name, _ in model.named_parameters()]\n",
    "    return master_params\n",
    "\n",
    "\n",
    "def zero_master_grads(master_params):\n",
    "    for param in master_params:\n",
    "        param.grad = None\n",
    "\n",
    "\n",
    "def zero_grad(model_params):\n",
    "    for param in model_params:\n",
    "        # Taken from https://pytorch.org/docs/stable/_modules/torch/optim/optimizer.html#Optimizer.add_param_group\n",
    "        if param.grad is not None:\n",
    "            param.grad.detach_()\n",
    "            param.grad.zero_()\n",
    "\n",
    "\n",
    "def param_grad_or_zeros(param):\n",
    "    if param.grad is not None:\n",
    "        return param.grad.data.detach()\n",
    "    else:\n",
    "        return th.zeros_like(param)\n",
    "\n",
    "\n",
    "class MixedPrecisionTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        model,\n",
    "        use_fp16=False,\n",
    "        fp16_scale_growth=1e-3,\n",
    "        initial_lg_loss_scale=INITIAL_LOG_LOSS_SCALE,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.use_fp16 = use_fp16\n",
    "        self.fp16_scale_growth = fp16_scale_growth\n",
    "\n",
    "        self.model_params = list(self.model.parameters())\n",
    "        self.master_params = self.model_params\n",
    "        self.param_groups_and_shapes = None\n",
    "        self.lg_loss_scale = initial_lg_loss_scale\n",
    "\n",
    "        if self.use_fp16:\n",
    "            self.param_groups_and_shapes = get_param_groups_and_shapes(\n",
    "                self.model.named_parameters()\n",
    "            )\n",
    "            self.master_params = make_master_params(self.param_groups_and_shapes)\n",
    "            self.model.convert_to_fp16()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        zero_grad(self.model_params)\n",
    "\n",
    "    def backward(self, loss: th.Tensor):\n",
    "        if self.use_fp16:\n",
    "            loss_scale = 2**self.lg_loss_scale\n",
    "            (loss * loss_scale).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "    def optimize(self, opt: th.optim.Optimizer):\n",
    "        if self.use_fp16:\n",
    "            return self._optimize_fp16(opt)\n",
    "        else:\n",
    "            return self._optimize_normal(opt)\n",
    "\n",
    "    def _optimize_fp16(self, opt: th.optim.Optimizer):\n",
    "        logger.logkv_mean(\"lg_loss_scale\", self.lg_loss_scale)\n",
    "        model_grads_to_master_grads(self.param_groups_and_shapes, self.master_params)\n",
    "        grad_norm, param_norm = self._compute_norms(grad_scale=2**self.lg_loss_scale)\n",
    "        if check_overflow(grad_norm):\n",
    "            self.lg_loss_scale -= 1\n",
    "            logger.log(f\"Found NaN, decreased lg_loss_scale to {self.lg_loss_scale}\")\n",
    "            zero_master_grads(self.master_params)\n",
    "            return False\n",
    "\n",
    "        logger.logkv_mean(\"grad_norm\", grad_norm)\n",
    "        logger.logkv_mean(\"param_norm\", param_norm)\n",
    "\n",
    "        for p in self.master_params:\n",
    "            p.grad.mul_(1.0 / (2**self.lg_loss_scale))\n",
    "        opt.step()\n",
    "        zero_master_grads(self.master_params)\n",
    "        master_params_to_model_params(self.param_groups_and_shapes, self.master_params)\n",
    "        self.lg_loss_scale += self.fp16_scale_growth\n",
    "        return True\n",
    "\n",
    "    def _optimize_normal(self, opt: th.optim.Optimizer):\n",
    "        grad_norm, param_norm = self._compute_norms()\n",
    "        logger.logkv_mean(\"grad_norm\", grad_norm)\n",
    "        logger.logkv_mean(\"param_norm\", param_norm)\n",
    "        opt.step()\n",
    "        return True\n",
    "\n",
    "    def _compute_norms(self, grad_scale=1.0):\n",
    "        grad_norm = 0.0\n",
    "        param_norm = 0.0\n",
    "        for p in self.master_params:\n",
    "            with th.no_grad():\n",
    "                param_norm += th.norm(p, p=2, dtype=th.float32).item() ** 2\n",
    "                if p.grad is not None:\n",
    "                    grad_norm += th.norm(p.grad, p=2, dtype=th.float32).item() ** 2\n",
    "        return np.sqrt(grad_norm) / grad_scale, np.sqrt(param_norm)\n",
    "\n",
    "    def master_params_to_state_dict(self, master_params):\n",
    "        return master_params_to_state_dict(\n",
    "            self.model, self.param_groups_and_shapes, master_params, self.use_fp16\n",
    "        )\n",
    "\n",
    "    def state_dict_to_master_params(self, state_dict):\n",
    "        return state_dict_to_master_params(self.model, state_dict, self.use_fp16)\n",
    "\n",
    "\n",
    "def check_overflow(value):\n",
    "    return (value == float(\"inf\")) or (value == -float(\"inf\")) or (value != value)\n",
    "\n",
    "\n",
    "class AttentionPool2d(nn.Module):\n",
    "    \"\"\"Adapted from CLIP: https://github.com/openai/CLIP/blob/main/clip/model.py.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        spacial_dim: int,\n",
    "        embed_dim: int,\n",
    "        num_heads_channels: int,\n",
    "        output_dim: int = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.positional_embedding = nn.Parameter(\n",
    "            th.randn(embed_dim, spacial_dim**2 + 1) / embed_dim**0.5\n",
    "        )\n",
    "        self.qkv_proj = conv_nd(1, embed_dim, 3 * embed_dim, 1)\n",
    "        self.c_proj = conv_nd(1, embed_dim, output_dim or embed_dim, 1)\n",
    "        self.num_heads = embed_dim // num_heads_channels\n",
    "        self.attention = QKVAttention(self.num_heads)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, *_spatial = x.shape\n",
    "        x = x.reshape(b, c, -1)  # NC(HW)\n",
    "        x = th.cat([x.mean(dim=-1, keepdim=True), x], dim=-1)  # NC(HW+1)\n",
    "        x = x + self.positional_embedding[None, :, :].to(x.dtype)  # NC(HW+1)\n",
    "        x = self.qkv_proj(x)\n",
    "        x = self.attention(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x[:, :, 0]\n",
    "\n",
    "\n",
    "class TimestepBlock(nn.Module):\n",
    "    \"\"\"Any module where forward() takes timestep embeddings as a second argument.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x, emb):\n",
    "        \"\"\"Apply the module to `x` given `emb` timestep embeddings.\"\"\"\n",
    "\n",
    "\n",
    "class TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n",
    "    \"\"\"A sequential module that passes timestep embeddings to the children that support it as an\n",
    "    extra input.\"\"\"\n",
    "\n",
    "    def forward(self, x, emb):\n",
    "        for layer in self:\n",
    "            if isinstance(layer, TimestepBlock):\n",
    "                x = layer(x, emb)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    \"\"\"An upsampling layer with an optional convolution.\n",
    "\n",
    "    :param channels: channels in the inputs and outputs.\n",
    "    :param use_conv: a bool determining if a convolution is applied.\n",
    "    :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then upsampling occurs in the\n",
    "        inner-two dimensions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, use_conv, dims=2, out_channels=None):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.out_channels = out_channels or channels\n",
    "        self.use_conv = use_conv\n",
    "        self.dims = dims\n",
    "        if use_conv:\n",
    "            self.conv = conv_nd(dims, self.channels, self.out_channels, 3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.shape[1] == self.channels\n",
    "        if self.dims == 3:\n",
    "            x = F.interpolate(x, (x.shape[2], x.shape[3] * 2, x.shape[4] * 2), mode=\"nearest\")\n",
    "        else:\n",
    "            x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "        if self.use_conv:\n",
    "            x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    \"\"\"A downsampling layer with an optional convolution.\n",
    "\n",
    "    :param channels: channels in the inputs and outputs.\n",
    "    :param use_conv: a bool determining if a convolution is applied.\n",
    "    :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then downsampling occurs in the\n",
    "        inner-two dimensions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, use_conv, dims=2, out_channels=None):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.out_channels = out_channels or channels\n",
    "        self.use_conv = use_conv\n",
    "        self.dims = dims\n",
    "        stride = 2 if dims != 3 else (1, 2, 2)\n",
    "        if use_conv:\n",
    "            self.op = conv_nd(dims, self.channels, self.out_channels, 3, stride=stride, padding=1)\n",
    "        else:\n",
    "            assert self.channels == self.out_channels\n",
    "            self.op = avg_pool_nd(dims, kernel_size=stride, stride=stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.shape[1] == self.channels\n",
    "        return self.op(x)\n",
    "\n",
    "\n",
    "class ResBlock(TimestepBlock):\n",
    "    \"\"\"A residual block that can optionally change the number of channels.\n",
    "\n",
    "    :param channels: the number of input channels.\n",
    "    :param emb_channels: the number of timestep embedding channels.\n",
    "    :param dropout: the rate of dropout.\n",
    "    :param out_channels: if specified, the number of out channels.\n",
    "    :param use_conv: if True and out_channels is specified, use a spatial convolution instead of a\n",
    "        smaller 1x1 convolution to change the channels in the skip connection.\n",
    "    :param dims: determines if the signal is 1D, 2D, or 3D.\n",
    "    :param use_checkpoint: if True, use gradient checkpointing on this module.\n",
    "    :param up: if True, use this block for upsampling.\n",
    "    :param down: if True, use this block for downsampling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels,\n",
    "        emb_channels,\n",
    "        dropout,\n",
    "        out_channels=None,\n",
    "        use_conv=False,\n",
    "        use_scale_shift_norm=False,\n",
    "        dims=2,\n",
    "        use_checkpoint=False,\n",
    "        up=False,\n",
    "        down=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.emb_channels = emb_channels\n",
    "        self.dropout = dropout\n",
    "        self.out_channels = out_channels or channels\n",
    "        self.use_conv = use_conv\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        self.use_scale_shift_norm = use_scale_shift_norm\n",
    "\n",
    "        self.in_layers = nn.Sequential(\n",
    "            normalization(channels),\n",
    "            nn.SiLU(),\n",
    "            conv_nd(dims, channels, self.out_channels, 3, padding=1),\n",
    "        )\n",
    "\n",
    "        self.updown = up or down\n",
    "\n",
    "        if up:\n",
    "            self.h_upd = Upsample(channels, False, dims)\n",
    "            self.x_upd = Upsample(channels, False, dims)\n",
    "        elif down:\n",
    "            self.h_upd = Downsample(channels, False, dims)\n",
    "            self.x_upd = Downsample(channels, False, dims)\n",
    "        else:\n",
    "            self.h_upd = self.x_upd = nn.Identity()\n",
    "\n",
    "        self.emb_layers = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            linear(\n",
    "                emb_channels,\n",
    "                2 * self.out_channels if use_scale_shift_norm else self.out_channels,\n",
    "            ),\n",
    "        )\n",
    "        self.out_layers = nn.Sequential(\n",
    "            normalization(self.out_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            zero_module(conv_nd(dims, self.out_channels, self.out_channels, 3, padding=1)),\n",
    "        )\n",
    "\n",
    "        if self.out_channels == channels:\n",
    "            self.skip_connection = nn.Identity()\n",
    "        elif use_conv:\n",
    "            self.skip_connection = conv_nd(dims, channels, self.out_channels, 3, padding=1)\n",
    "        else:\n",
    "            self.skip_connection = conv_nd(dims, channels, self.out_channels, 1)\n",
    "\n",
    "    def forward(self, x, emb):\n",
    "        \"\"\"Apply the block to a Tensor, conditioned on a timestep embedding.\n",
    "\n",
    "        :param x: an [N x C x ...] Tensor of features.\n",
    "        :param emb: an [N x emb_channels] Tensor of timestep embeddings.\n",
    "        :return: an [N x C x ...] Tensor of outputs.\n",
    "        \"\"\"\n",
    "        return checkpoint(self._forward, (x, emb), self.parameters(), self.use_checkpoint)\n",
    "\n",
    "    def _forward(self, x, emb):\n",
    "        if self.updown:\n",
    "            in_rest, in_conv = self.in_layers[:-1], self.in_layers[-1]\n",
    "            h = in_rest(x)\n",
    "            h = self.h_upd(h)\n",
    "            x = self.x_upd(x)\n",
    "            h = in_conv(h)\n",
    "        else:\n",
    "            h = self.in_layers(x)\n",
    "        emb_out = self.emb_layers(emb).type(h.dtype)\n",
    "        while len(emb_out.shape) < len(h.shape):\n",
    "            emb_out = emb_out[..., None]\n",
    "        if self.use_scale_shift_norm:\n",
    "            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n",
    "            scale, shift = th.chunk(emb_out, 2, dim=1)\n",
    "            h = out_norm(h) * (1 + scale) + shift\n",
    "            h = out_rest(h)\n",
    "        else:\n",
    "            h = h + emb_out\n",
    "            h = self.out_layers(h)\n",
    "        return self.skip_connection(x) + h\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"An attention block that allows spatial positions to attend to each other.\n",
    "\n",
    "    Originally ported from here, but adapted to the N-d case.\n",
    "    https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/models/unet.py#L66.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels,\n",
    "        num_heads=1,\n",
    "        num_head_channels=-1,\n",
    "        use_checkpoint=False,\n",
    "        use_new_attention_order=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        if num_head_channels == -1:\n",
    "            self.num_heads = num_heads\n",
    "        else:\n",
    "            assert (\n",
    "                channels % num_head_channels == 0\n",
    "            ), f\"q,k,v channels {channels} is not divisible by num_head_channels {num_head_channels}\"\n",
    "            self.num_heads = channels // num_head_channels\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        self.norm = normalization(channels)\n",
    "        self.qkv = conv_nd(1, channels, channels * 3, 1)\n",
    "        if use_new_attention_order:\n",
    "            # split qkv before split heads\n",
    "            self.attention = QKVAttention(self.num_heads)\n",
    "        else:\n",
    "            # split heads before split qkv\n",
    "            self.attention = QKVAttentionLegacy(self.num_heads)\n",
    "\n",
    "        self.proj_out = zero_module(conv_nd(1, channels, channels, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return checkpoint(self._forward, (x,), self.parameters(), self.use_checkpoint)\n",
    "\n",
    "    def _forward(self, x):\n",
    "        b, c, *spatial = x.shape\n",
    "        x = x.reshape(b, c, -1)\n",
    "        qkv = self.qkv(self.norm(x))\n",
    "        h = self.attention(qkv)\n",
    "        h = self.proj_out(h)\n",
    "        return (x + h).reshape(b, c, *spatial)\n",
    "\n",
    "\n",
    "def count_flops_attn(model, _x, y):\n",
    "    \"\"\"A counter for the `thop` package to count the operations in an attention operation.\n",
    "\n",
    "    Meant to be used like:\n",
    "        macs, params = thop.profile(\n",
    "            model,\n",
    "            inputs=(inputs, timestamps),\n",
    "            custom_ops={QKVAttention: QKVAttention.count_flops},\n",
    "        )\n",
    "    \"\"\"\n",
    "    b, c, *spatial = y[0].shape\n",
    "    num_spatial = int(np.prod(spatial))\n",
    "    # We perform two matmuls with the same number of ops.\n",
    "    # The first computes the weight matrix, the second computes\n",
    "    # the combination of the value vectors.\n",
    "    matmul_ops = 2 * b * (num_spatial**2) * c\n",
    "    model.total_ops += th.DoubleTensor([matmul_ops])\n",
    "\n",
    "\n",
    "class QKVAttentionLegacy(nn.Module):\n",
    "    \"\"\"A module which performs QKV attention.\n",
    "\n",
    "    Matches legacy QKVAttention + input/output heads shaping\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_heads):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, qkv):\n",
    "        \"\"\"Apply QKV attention.\n",
    "\n",
    "        :param qkv: an [N x (H * 3 * C) x T] tensor of Qs, Ks, and Vs.\n",
    "        :return: an [N x (H * C) x T] tensor after attention.\n",
    "        \"\"\"\n",
    "        bs, width, length = qkv.shape\n",
    "        assert width % (3 * self.n_heads) == 0\n",
    "        ch = width // (3 * self.n_heads)\n",
    "        q, k, v = qkv.reshape(bs * self.n_heads, ch * 3, length).split(ch, dim=1)\n",
    "        scale = 1 / math.sqrt(math.sqrt(ch))\n",
    "        weight = th.einsum(\n",
    "            \"bct,bcs->bts\", q * scale, k * scale\n",
    "        )  # More stable with f16 than dividing afterwards\n",
    "        weight = th.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
    "        a = th.einsum(\"bts,bcs->bct\", weight, v)\n",
    "        return a.reshape(bs, -1, length)\n",
    "\n",
    "    @staticmethod\n",
    "    def count_flops(model, _x, y):\n",
    "        return count_flops_attn(model, _x, y)\n",
    "\n",
    "\n",
    "class QKVAttention(nn.Module):\n",
    "    \"\"\"A module which performs QKV attention and splits in a different order.\"\"\"\n",
    "\n",
    "    def __init__(self, n_heads):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, qkv):\n",
    "        \"\"\"Apply QKV attention.\n",
    "\n",
    "        :param qkv: an [N x (3 * H * C) x T] tensor of Qs, Ks, and Vs.\n",
    "        :return: an [N x (H * C) x T] tensor after attention.\n",
    "        \"\"\"\n",
    "        bs, width, length = qkv.shape\n",
    "        assert width % (3 * self.n_heads) == 0\n",
    "        ch = width // (3 * self.n_heads)\n",
    "        q, k, v = qkv.chunk(3, dim=1)\n",
    "        scale = 1 / math.sqrt(math.sqrt(ch))\n",
    "        weight = th.einsum(\n",
    "            \"bct,bcs->bts\",\n",
    "            (q * scale).view(bs * self.n_heads, ch, length),\n",
    "            (k * scale).view(bs * self.n_heads, ch, length),\n",
    "        )  # More stable with f16 than dividing afterwards\n",
    "        weight = th.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
    "        a = th.einsum(\"bts,bcs->bct\", weight, v.reshape(bs * self.n_heads, ch, length))\n",
    "        return a.reshape(bs, -1, length)\n",
    "\n",
    "    @staticmethod\n",
    "    def count_flops(model, _x, y):\n",
    "        return count_flops_attn(model, _x, y)\n",
    "\n",
    "\n",
    "class EncoderUNetModel(nn.Module):\n",
    "    \"\"\"The half UNet model with attention and timestep embedding.\n",
    "\n",
    "    For usage, see UNet.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size,\n",
    "        in_channels,\n",
    "        model_channels,\n",
    "        out_channels,\n",
    "        num_res_blocks,\n",
    "        attention_resolutions,\n",
    "        dropout=0,\n",
    "        channel_mult=(1, 2, 4, 8),\n",
    "        conv_resample=True,\n",
    "        dims=2,\n",
    "        use_checkpoint=False,\n",
    "        use_fp16=False,\n",
    "        num_heads=1,\n",
    "        num_head_channels=-1,\n",
    "        num_heads_upsample=-1,\n",
    "        use_scale_shift_norm=False,\n",
    "        resblock_updown=False,\n",
    "        use_new_attention_order=False,\n",
    "        pool=\"adaptive\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if num_heads_upsample == -1:\n",
    "            num_heads_upsample = num_heads\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.model_channels = model_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        self.attention_resolutions = attention_resolutions\n",
    "        self.dropout = dropout\n",
    "        self.channel_mult = channel_mult\n",
    "        self.conv_resample = conv_resample\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        self.dtype = th.float16 if use_fp16 else th.float32\n",
    "        self.num_heads = num_heads\n",
    "        self.num_head_channels = num_head_channels\n",
    "        self.num_heads_upsample = num_heads_upsample\n",
    "\n",
    "        time_embed_dim = model_channels * 4\n",
    "        self.time_embed = nn.Sequential(\n",
    "            linear(model_channels, time_embed_dim),\n",
    "            nn.SiLU(),\n",
    "            linear(time_embed_dim, time_embed_dim),\n",
    "        )\n",
    "\n",
    "        ch = int(channel_mult[0] * model_channels)\n",
    "        self.input_blocks = nn.ModuleList(\n",
    "            [TimestepEmbedSequential(conv_nd(dims, in_channels, ch, 3, padding=1))]\n",
    "        )\n",
    "        self._feature_size = ch\n",
    "        input_block_chans = [ch]\n",
    "        ds = 1\n",
    "        for level, mult in enumerate(channel_mult):\n",
    "            for _ in range(num_res_blocks):\n",
    "                layers = [\n",
    "                    ResBlock(\n",
    "                        ch,\n",
    "                        time_embed_dim,\n",
    "                        dropout,\n",
    "                        out_channels=int(mult * model_channels),\n",
    "                        dims=dims,\n",
    "                        use_checkpoint=use_checkpoint,\n",
    "                        use_scale_shift_norm=use_scale_shift_norm,\n",
    "                    )\n",
    "                ]\n",
    "                ch = int(mult * model_channels)\n",
    "                if ds in attention_resolutions:\n",
    "                    layers.append(\n",
    "                        AttentionBlock(\n",
    "                            ch,\n",
    "                            use_checkpoint=use_checkpoint,\n",
    "                            num_heads=num_heads,\n",
    "                            num_head_channels=num_head_channels,\n",
    "                            use_new_attention_order=use_new_attention_order,\n",
    "                        )\n",
    "                    )\n",
    "                self.input_blocks.append(TimestepEmbedSequential(*layers))\n",
    "                self._feature_size += ch\n",
    "                input_block_chans.append(ch)\n",
    "            if level != len(channel_mult) - 1:\n",
    "                out_ch = ch\n",
    "                self.input_blocks.append(\n",
    "                    TimestepEmbedSequential(\n",
    "                        ResBlock(\n",
    "                            ch,\n",
    "                            time_embed_dim,\n",
    "                            dropout,\n",
    "                            out_channels=out_ch,\n",
    "                            dims=dims,\n",
    "                            use_checkpoint=use_checkpoint,\n",
    "                            use_scale_shift_norm=use_scale_shift_norm,\n",
    "                            down=True,\n",
    "                        )\n",
    "                        if resblock_updown\n",
    "                        else Downsample(ch, conv_resample, dims=dims, out_channels=out_ch)\n",
    "                    )\n",
    "                )\n",
    "                ch = out_ch\n",
    "                input_block_chans.append(ch)\n",
    "                ds *= 2\n",
    "                self._feature_size += ch\n",
    "\n",
    "        self.middle_block = TimestepEmbedSequential(\n",
    "            ResBlock(\n",
    "                ch,\n",
    "                time_embed_dim,\n",
    "                dropout,\n",
    "                dims=dims,\n",
    "                use_checkpoint=use_checkpoint,\n",
    "                use_scale_shift_norm=use_scale_shift_norm,\n",
    "            ),\n",
    "            AttentionBlock(\n",
    "                ch,\n",
    "                use_checkpoint=use_checkpoint,\n",
    "                num_heads=num_heads,\n",
    "                num_head_channels=num_head_channels,\n",
    "                use_new_attention_order=use_new_attention_order,\n",
    "            ),\n",
    "            ResBlock(\n",
    "                ch,\n",
    "                time_embed_dim,\n",
    "                dropout,\n",
    "                dims=dims,\n",
    "                use_checkpoint=use_checkpoint,\n",
    "                use_scale_shift_norm=use_scale_shift_norm,\n",
    "            ),\n",
    "        )\n",
    "        self._feature_size += ch\n",
    "        self.pool = pool\n",
    "        if pool == \"adaptive\":\n",
    "            self.out = nn.Sequential(\n",
    "                normalization(ch),\n",
    "                nn.SiLU(),\n",
    "                nn.AdaptiveAvgPool2d((1, 1)),\n",
    "                zero_module(conv_nd(dims, ch, out_channels, 1)),\n",
    "                nn.Flatten(),\n",
    "            )\n",
    "        elif pool == \"attention\":\n",
    "            assert num_head_channels != -1\n",
    "            self.out = nn.Sequential(\n",
    "                normalization(ch),\n",
    "                nn.SiLU(),\n",
    "                AttentionPool2d((image_size // ds), ch, num_head_channels, out_channels),\n",
    "            )\n",
    "        elif pool == \"spatial\":\n",
    "            self.out = nn.Sequential(\n",
    "                nn.Linear(self._feature_size, 2048),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(2048, self.out_channels),\n",
    "            )\n",
    "        elif pool == \"spatial_v2\":\n",
    "            self.out = nn.Sequential(\n",
    "                nn.Linear(self._feature_size, 2048),\n",
    "                normalization(2048),\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(2048, self.out_channels),\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Unexpected {pool} pooling\")\n",
    "\n",
    "    def convert_to_fp16(self):\n",
    "        \"\"\"Convert the torso of the model to float16.\"\"\"\n",
    "        self.input_blocks.apply(convert_module_to_f16)\n",
    "        self.middle_block.apply(convert_module_to_f16)\n",
    "\n",
    "    def convert_to_fp32(self):\n",
    "        \"\"\"Convert the torso of the model to float32.\"\"\"\n",
    "        self.input_blocks.apply(convert_module_to_f32)\n",
    "        self.middle_block.apply(convert_module_to_f32)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Apply the model to an input batch.\n",
    "\n",
    "        :param x: an [N x C x ...] Tensor of inputs.\n",
    "        :param timesteps: a 1-D batch of timesteps.\n",
    "        :return: an [N x K] Tensor of outputs.\n",
    "        \"\"\"\n",
    "        x = inputs[:,1:].reshape((-1, 1, 28, 28))\n",
    "        timesteps = inputs[:,0]\n",
    "        \n",
    "        emb = self.time_embed(timestep_embedding(timesteps, self.model_channels))\n",
    "\n",
    "        results = []\n",
    "        h = x.type(self.dtype)\n",
    "        for module in self.input_blocks:\n",
    "            h = module(h, emb)\n",
    "            if self.pool.startswith(\"spatial\"):\n",
    "                results.append(h.type(x.dtype).mean(dim=(2, 3)))\n",
    "        h = self.middle_block(h, emb)\n",
    "        if self.pool.startswith(\"spatial\"):\n",
    "            results.append(h.type(x.dtype).mean(dim=(2, 3)))\n",
    "            h = th.cat(results, axis=-1)\n",
    "            return self.out(h)\n",
    "        else:\n",
    "            h = h.type(x.dtype)\n",
    "            return torch.hstack((inputs,self.out(h)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "NUM_CLASSES = 1000\n",
    "\n",
    "\n",
    "class UNetModelKoopman(nn.Module):\n",
    "    \"\"\"The full UNet model with attention and timestep embedding.\n",
    "\n",
    "    :param in_channels: channels in the input Tensor.\n",
    "    :param model_channels: base channel count for the model.\n",
    "    :param out_channels: channels in the output Tensor.\n",
    "    :param num_res_blocks: number of residual blocks per downsample.\n",
    "    :param attention_resolutions: a collection of downsample rates at which\n",
    "        attention will take place. May be a set, list, or tuple.\n",
    "        For example, if this contains 4, then at 4x downsampling, attention\n",
    "        will be used.\n",
    "    :param dropout: the dropout probability.\n",
    "    :param channel_mult: channel multiplier for each level of the UNet.\n",
    "    :param conv_resample: if True, use learned convolutions for upsampling and\n",
    "        downsampling.\n",
    "    :param dims: determines if the signal is 1D, 2D, or 3D.\n",
    "    :param num_classes: if specified (as an int), then this model will be\n",
    "        class-conditional with `num_classes` classes.\n",
    "    :param use_checkpoint: use gradient checkpointing to reduce memory usage.\n",
    "    :param num_heads: the number of attention heads in each attention layer.\n",
    "    :param num_heads_channels: if specified, ignore num_heads and instead use\n",
    "                               a fixed channel width per attention head.\n",
    "    :param num_heads_upsample: works with num_heads to set a different number\n",
    "                               of heads for upsampling. Deprecated.\n",
    "    :param use_scale_shift_norm: use a FiLM-like conditioning mechanism.\n",
    "    :param resblock_updown: use residual blocks for up/downsampling.\n",
    "    :param use_new_attention_order: use a different attention pattern for potentially\n",
    "                                    increased efficiency.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size,\n",
    "        in_channels,\n",
    "        model_channels,\n",
    "        out_channels,\n",
    "        num_res_blocks,\n",
    "        attention_resolutions,\n",
    "        dropout=0,\n",
    "        channel_mult=(1, 2, 4, 8),\n",
    "        conv_resample=True,\n",
    "        dims=2,\n",
    "        num_classes=None,\n",
    "        use_checkpoint=False,\n",
    "        use_fp16=False,\n",
    "        num_heads=1,\n",
    "        num_head_channels=-1,\n",
    "        num_heads_upsample=-1,\n",
    "        use_scale_shift_norm=False,\n",
    "        resblock_updown=False,\n",
    "        use_new_attention_order=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if num_heads_upsample == -1:\n",
    "            num_heads_upsample = num_heads\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.in_channels = in_channels\n",
    "        self.model_channels = model_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        self.attention_resolutions = attention_resolutions\n",
    "        self.dropout = dropout\n",
    "        self.channel_mult = channel_mult\n",
    "        self.conv_resample = conv_resample\n",
    "        self.num_classes = num_classes\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        self.dtype = th.float16 if use_fp16 else th.float32\n",
    "        self.num_heads = num_heads\n",
    "        self.num_head_channels = num_head_channels\n",
    "        self.num_heads_upsample = num_heads_upsample\n",
    "\n",
    "        time_embed_dim = model_channels * 4\n",
    "        self.time_embed = nn.Sequential(\n",
    "            linear(model_channels, time_embed_dim),\n",
    "            nn.SiLU(),\n",
    "            linear(time_embed_dim, time_embed_dim),\n",
    "        )\n",
    "\n",
    "        if self.num_classes is not None:\n",
    "            self.label_emb = nn.Embedding(num_classes, time_embed_dim)\n",
    "\n",
    "        ch = input_ch = int(channel_mult[0] * model_channels)\n",
    "        self.input_blocks = nn.ModuleList(\n",
    "            [TimestepEmbedSequential(conv_nd(dims, in_channels, ch, 3, padding=1))]\n",
    "        )\n",
    "        self._feature_size = ch\n",
    "        input_block_chans = [ch]\n",
    "        ds = 1\n",
    "        for level, mult in enumerate(channel_mult):\n",
    "            for _ in range(num_res_blocks):\n",
    "                layers = [\n",
    "                    ResBlock(\n",
    "                        ch,\n",
    "                        time_embed_dim,\n",
    "                        dropout,\n",
    "                        out_channels=int(mult * model_channels),\n",
    "                        dims=dims,\n",
    "                        use_checkpoint=use_checkpoint,\n",
    "                        use_scale_shift_norm=use_scale_shift_norm,\n",
    "                    )\n",
    "                ]\n",
    "                ch = int(mult * model_channels)\n",
    "                if ds in attention_resolutions:\n",
    "                    layers.append(\n",
    "                        AttentionBlock(\n",
    "                            ch,\n",
    "                            use_checkpoint=use_checkpoint,\n",
    "                            num_heads=num_heads,\n",
    "                            num_head_channels=num_head_channels,\n",
    "                            use_new_attention_order=use_new_attention_order,\n",
    "                        )\n",
    "                    )\n",
    "                self.input_blocks.append(TimestepEmbedSequential(*layers))\n",
    "                self._feature_size += ch\n",
    "                input_block_chans.append(ch)\n",
    "            if level != len(channel_mult) - 1:\n",
    "                out_ch = ch\n",
    "                self.input_blocks.append(\n",
    "                    TimestepEmbedSequential(\n",
    "                        ResBlock(\n",
    "                            ch,\n",
    "                            time_embed_dim,\n",
    "                            dropout,\n",
    "                            out_channels=out_ch,\n",
    "                            dims=dims,\n",
    "                            use_checkpoint=use_checkpoint,\n",
    "                            use_scale_shift_norm=use_scale_shift_norm,\n",
    "                            down=True,\n",
    "                        )\n",
    "                        if resblock_updown\n",
    "                        else Downsample(ch, conv_resample, dims=dims, out_channels=out_ch)\n",
    "                    )\n",
    "                )\n",
    "                ch = out_ch\n",
    "                input_block_chans.append(ch)\n",
    "                ds *= 2\n",
    "                self._feature_size += ch\n",
    "\n",
    "        self.middle_block = TimestepEmbedSequential(\n",
    "            ResBlock(\n",
    "                ch,\n",
    "                time_embed_dim,\n",
    "                dropout,\n",
    "                dims=dims,\n",
    "                use_checkpoint=use_checkpoint,\n",
    "                use_scale_shift_norm=use_scale_shift_norm,\n",
    "            ),\n",
    "            AttentionBlock(\n",
    "                ch,\n",
    "                use_checkpoint=use_checkpoint,\n",
    "                num_heads=num_heads,\n",
    "                num_head_channels=num_head_channels,\n",
    "                use_new_attention_order=use_new_attention_order,\n",
    "            ),\n",
    "            ResBlock(\n",
    "                ch,\n",
    "                time_embed_dim,\n",
    "                dropout,\n",
    "                dims=dims,\n",
    "                use_checkpoint=use_checkpoint,\n",
    "                use_scale_shift_norm=use_scale_shift_norm,\n",
    "            ),\n",
    "        )\n",
    "        self._feature_size += ch\n",
    "\n",
    "        self.output_blocks = nn.ModuleList([])\n",
    "        for level, mult in list(enumerate(channel_mult))[::-1]:\n",
    "            for i in range(num_res_blocks + 1):\n",
    "                ich = input_block_chans.pop()\n",
    "                layers = [\n",
    "                    ResBlock(\n",
    "                        ch + ich,\n",
    "                        time_embed_dim,\n",
    "                        dropout,\n",
    "                        out_channels=int(model_channels * mult),\n",
    "                        dims=dims,\n",
    "                        use_checkpoint=use_checkpoint,\n",
    "                        use_scale_shift_norm=use_scale_shift_norm,\n",
    "                    )\n",
    "                ]\n",
    "                ch = int(model_channels * mult)\n",
    "                if ds in attention_resolutions:\n",
    "                    layers.append(\n",
    "                        AttentionBlock(\n",
    "                            ch,\n",
    "                            use_checkpoint=use_checkpoint,\n",
    "                            num_heads=num_heads_upsample,\n",
    "                            num_head_channels=num_head_channels,\n",
    "                            use_new_attention_order=use_new_attention_order,\n",
    "                        )\n",
    "                    )\n",
    "                if level and i == num_res_blocks:\n",
    "                    out_ch = ch\n",
    "                    layers.append(\n",
    "                        ResBlock(\n",
    "                            ch,\n",
    "                            time_embed_dim,\n",
    "                            dropout,\n",
    "                            out_channels=out_ch,\n",
    "                            dims=dims,\n",
    "                            use_checkpoint=use_checkpoint,\n",
    "                            use_scale_shift_norm=use_scale_shift_norm,\n",
    "                            up=True,\n",
    "                        )\n",
    "                        if resblock_updown\n",
    "                        else Upsample(ch, conv_resample, dims=dims, out_channels=out_ch)\n",
    "                    )\n",
    "                    ds //= 2\n",
    "                self.output_blocks.append(TimestepEmbedSequential(*layers))\n",
    "                self._feature_size += ch\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            normalization(ch),\n",
    "            nn.SiLU(),\n",
    "            zero_module(conv_nd(dims, input_ch, out_channels, 3, padding=1)),\n",
    "        )\n",
    "\n",
    "    def convert_to_fp16(self):\n",
    "        \"\"\"Convert the torso of the model to float16.\"\"\"\n",
    "        self.input_blocks.apply(convert_module_to_f16)\n",
    "        self.middle_block.apply(convert_module_to_f16)\n",
    "        self.output_blocks.apply(convert_module_to_f16)\n",
    "\n",
    "    def convert_to_fp32(self):\n",
    "        \"\"\"Convert the torso of the model to float32.\"\"\"\n",
    "        self.input_blocks.apply(convert_module_to_f32)\n",
    "        self.middle_block.apply(convert_module_to_f32)\n",
    "        self.output_blocks.apply(convert_module_to_f32)\n",
    "\n",
    "    def forward(self, t, x, y=None):\n",
    "        \"\"\"Apply the model to an input batch.\n",
    "\n",
    "        :param x: an [N x C x ...] Tensor of inputs.\n",
    "        :param timesteps: a 1-D batch of timesteps.\n",
    "        :param y: an [N] Tensor of labels, if class-conditional.\n",
    "        :return: an [N x C x ...] Tensor of outputs.\n",
    "        \"\"\"\n",
    "        timesteps = t\n",
    "        assert (y is not None) == (\n",
    "            self.num_classes is not None\n",
    "        ), \"must specify y if and only if the model is class-conditional\"\n",
    "        while timesteps.dim() > 1:\n",
    "            timesteps = timesteps[:, 0]\n",
    "        if timesteps.dim() == 0:\n",
    "            timesteps = timesteps.repeat(x.shape[0])\n",
    "\n",
    "        hs = []\n",
    "        emb = self.time_embed(timestep_embedding(timesteps, self.model_channels))\n",
    "\n",
    "        if self.num_classes is not None:\n",
    "            assert y.shape == (x.shape[0],)\n",
    "            emb = emb + self.label_emb(y)\n",
    "\n",
    "        h = x.type(self.dtype)\n",
    "        for module in self.input_blocks:\n",
    "            h = module(h, emb)\n",
    "            hs.append(h)\n",
    "        h = self.middle_block(h, emb)\n",
    "        for module in self.output_blocks:\n",
    "            h = th.cat([h, hs.pop()], dim=1)\n",
    "            h = module(h, emb)\n",
    "        h = h.type(x.dtype)\n",
    "        out = self.out(h)\n",
    "        #reshape for better koopman\n",
    "        out = out.view(out.shape[0],-1)\n",
    "        out = torch.hstack((torch.ones(out.shape[0], 1, device=out.device), out))  \n",
    "        return out\n",
    "    \n",
    "class UNetModelWrapperKoopman(UNetModelKoopman):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_channels,\n",
    "        num_res_blocks,\n",
    "        channel_mult=None,\n",
    "        learn_sigma=False,\n",
    "        class_cond=False,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        use_checkpoint=False,\n",
    "        attention_resolutions=\"16\",\n",
    "        num_heads=1,\n",
    "        num_head_channels=-1,\n",
    "        num_heads_upsample=-1,\n",
    "        use_scale_shift_norm=False,\n",
    "        dropout=0,\n",
    "        resblock_updown=False,\n",
    "        use_fp16=False,\n",
    "        use_new_attention_order=False,\n",
    "    ):\n",
    "        \"\"\"Dim (tuple): (C, H, W)\"\"\"\n",
    "        image_size = dim[-1]\n",
    "        if channel_mult is None:\n",
    "            if image_size == 512:\n",
    "                channel_mult = (0.5, 1, 1, 2, 2, 4, 4)\n",
    "            elif image_size == 256:\n",
    "                channel_mult = (1, 1, 2, 2, 4, 4)\n",
    "            elif image_size == 128:\n",
    "                channel_mult = (1, 1, 2, 3, 4)\n",
    "            elif image_size == 64:\n",
    "                channel_mult = (1, 2, 3, 4)\n",
    "            elif image_size == 32:\n",
    "                channel_mult = (1, 2, 2, 2)\n",
    "            elif image_size == 28:\n",
    "                channel_mult = (1, 2, 2)\n",
    "            else:\n",
    "                raise ValueError(f\"unsupported image size: {image_size}\")\n",
    "        else:\n",
    "            channel_mult = list(channel_mult)\n",
    "\n",
    "        attention_ds = []\n",
    "        for res in attention_resolutions.split(\",\"):\n",
    "            attention_ds.append(image_size // int(res))\n",
    "\n",
    "        return super().__init__(\n",
    "            image_size=image_size,\n",
    "            in_channels=dim[0],\n",
    "            model_channels=num_channels,\n",
    "            out_channels=(dim[0] if not learn_sigma else dim[0] * 2),\n",
    "            num_res_blocks=num_res_blocks,\n",
    "            attention_resolutions=tuple(attention_ds),\n",
    "            dropout=dropout,\n",
    "            channel_mult=channel_mult,\n",
    "            num_classes=(num_classes if class_cond else None),\n",
    "            use_checkpoint=use_checkpoint,\n",
    "            use_fp16=use_fp16,\n",
    "            num_heads=num_heads,\n",
    "            num_head_channels=num_head_channels,\n",
    "            num_heads_upsample=num_heads_upsample,\n",
    "            use_scale_shift_norm=use_scale_shift_norm,\n",
    "            resblock_updown=resblock_updown,\n",
    "            use_new_attention_order=use_new_attention_order,\n",
    "        )\n",
    "\n",
    "    def forward(self, t, x, y=None, *args, **kwargs):\n",
    "        return super().forward(t, x, y=y)\n",
    "\n",
    "class EncoderUNetModelWrapper(EncoderUNetModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_channels,\n",
    "        num_res_blocks,\n",
    "        channel_mult=None,\n",
    "        dropout=0,\n",
    "        attention_resolutions=\"16\",\n",
    "        use_checkpoint=False,\n",
    "        use_fp16=False,\n",
    "        num_heads=1,\n",
    "        num_head_channels=-1,\n",
    "        num_heads_upsample=-1,\n",
    "        use_scale_shift_norm=False,\n",
    "        resblock_updown=False,\n",
    "        use_new_attention_order=False,\n",
    "        pool=\"adaptive\",\n",
    "    ):\n",
    "        \"\"\"Dim (tuple): (C, H, W)\"\"\"\n",
    "        image_size = dim[-1]\n",
    "\n",
    "        if channel_mult is None:\n",
    "            if image_size == 512:\n",
    "                channel_mult = (0.5, 1, 1, 2, 2, 4, 4)\n",
    "            elif image_size == 256:\n",
    "                channel_mult = (1, 1, 2, 2, 4, 4)\n",
    "            elif image_size == 128:\n",
    "                channel_mult = (1, 1, 2, 3, 4)\n",
    "            elif image_size == 64:\n",
    "                channel_mult = (1, 2, 3, 4)\n",
    "            elif image_size == 32:\n",
    "                channel_mult = (1, 2, 2, 2)\n",
    "            elif image_size == 28:\n",
    "                channel_mult = (1, 2, 2)\n",
    "            else:\n",
    "                raise ValueError(f\"unsupported image size: {image_size}\")\n",
    "\n",
    "        attention_ds = []\n",
    "        for res in attention_resolutions.split(\",\"):\n",
    "            attention_ds.append(image_size // int(res))\n",
    "\n",
    "        super().__init__(\n",
    "            image_size=image_size,\n",
    "            in_channels=dim[0],\n",
    "            model_channels=num_channels,\n",
    "            out_channels=num_channels,\n",
    "            num_res_blocks=num_res_blocks,\n",
    "            attention_resolutions=tuple(attention_ds),\n",
    "            dropout=dropout,\n",
    "            channel_mult=channel_mult,\n",
    "            conv_resample=True,\n",
    "            dims=2,\n",
    "            use_checkpoint=use_checkpoint,\n",
    "            use_fp16=use_fp16,\n",
    "            num_heads=num_heads,\n",
    "            num_head_channels=num_head_channels,\n",
    "            num_heads_upsample=num_heads_upsample,\n",
    "            use_scale_shift_norm=use_scale_shift_norm,\n",
    "            resblock_updown=resblock_updown,\n",
    "            use_new_attention_order=use_new_attention_order,\n",
    "            pool=pool,\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return super().forward(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full MNIST dataset: (why not?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full mnist reduce to 10 000 random samples \n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "\n",
    "trainset = datasets.MNIST(\n",
    "    \"../data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Randomly select 10,000 indices\n",
    "#total_samples = 10000\n",
    "#random_indices = torch.randperm(len(full_trainset))[:total_samples]\n",
    "#subset_dataset = Subset(trainset, random_indices)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(\n",
    "    trainset, batch_size=batch_size, shuffle=True, drop_last=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Let's plot some to make sure\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "# Get a batch from the train loader\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Denormalize images: [-1, 1] → [0, 1]\n",
    "images = images * 0.5 + 0.5\n",
    "\n",
    "# Create a grid of the first 16 images\n",
    "grid_img = vutils.make_grid(images[:16], nrow=4, padding=2)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(grid_img.permute(1, 2, 0).squeeze(), cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Sample MNIST Images (Random 10,000 Subset)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's learn the flow via CFM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "#            OT-CFM\n",
    "#################################\n",
    "\n",
    "\n",
    "n_epochs = 20\n",
    "\n",
    "sigma = 0.0\n",
    "model = UNetModel(dim=(1, 28, 28), num_channels=32, num_res_blocks=1).to(device) #for MNIST\n",
    "#model = UNetModel(dim=(1, 32, 32), num_channels=32, num_res_blocks=1).to(device) #for CIFAR-10\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "# FM = ConditionalFlowMatcher(sigma=sigma)\n",
    "FM = ExactOptimalTransportConditionalFlowMatcher(sigma=sigma)\n",
    "node = NeuralODE(model, solver=\"dopri5\", sensitivity=\"adjoint\", atol=1e-4, rtol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "468it [00:28, 16.64it/s]\n",
      "468it [00:28, 16.45it/s]\n",
      "468it [00:27, 16.85it/s]\n",
      "468it [00:28, 16.66it/s]\n",
      "468it [00:27, 16.82it/s]\n",
      "468it [00:28, 16.62it/s]\n",
      "468it [00:27, 16.97it/s]\n",
      "468it [00:27, 17.03it/s]\n",
      "468it [00:27, 16.96it/s]\n",
      "468it [00:28, 16.71it/s]\n",
      "468it [00:27, 16.78it/s]\n",
      "468it [00:27, 16.79it/s]\n",
      "468it [00:27, 16.77it/s]\n",
      "468it [00:28, 16.71it/s]\n",
      "468it [00:28, 16.58it/s]\n",
      "468it [00:27, 16.91it/s]\n",
      "468it [00:28, 16.49it/s]\n",
      "468it [00:27, 16.75it/s]\n",
      "468it [00:28, 16.58it/s]\n",
      "468it [00:27, 16.80it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    for i, data in tqdm(enumerate(train_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        x1 = data[0].to(device)\n",
    "        x0 = torch.randn_like(x1)\n",
    "        t, xt, ut = FM.sample_location_and_conditional_flow(x0, x1)\n",
    "        vt = model(t, xt)\n",
    "        loss = torch.mean((vt - ut) ** 2)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), \"/home/turan/koopman/cfm_model/unet_model_mnist.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"/home/turan/koopman/cfm_model/unet_model_mnist.pth\"))\n",
    "node = NeuralODE(model, solver=\"dopri5\", sensitivity=\"adjoint\", atol=1e-4, rtol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamics = UNetModelWrapperKoopman(dim=(1, 28, 28), num_channels=32, num_res_blocks=1).to(\"cpu\")\n",
    "dynamics.load_state_dict(torch.load(\"/home/turan/koopman/cfm_model/unet_model_mnist.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in dynamics.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling time: 5.345369338989258\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f664c657640>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Check that the learned flow generates ok samples\n",
    "with torch.no_grad():\n",
    "    top = time()\n",
    "    traj = node.trajectory(\n",
    "        torch.randn(10, 1, 28, 28, device=device),\n",
    "        t_span=torch.linspace(0, 1, 100, device=device),\n",
    "    )\n",
    "    print(f'Sampling time: {(time()-top)}')\n",
    "grid = make_grid(\n",
    "    traj[-1, :100].view([-1, 1, 28, 28]).clip(-1, 1), value_range=(-1, 1), padding=0, nrow=10\n",
    ")\n",
    "img = ToPILImage()(grid)\n",
    "plt.imshow(img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Koopman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetModel_encoder(nn.Module):\n",
    "    \"\"\"The full UNet model with attention and timestep embedding.\n",
    "\n",
    "    :param in_channels: channels in the input Tensor.\n",
    "    :param model_channels: base channel count for the model.\n",
    "    :param out_channels: channels in the output Tensor.\n",
    "    :param num_res_blocks: number of residual blocks per downsample.\n",
    "    :param attention_resolutions: a collection of downsample rates at which\n",
    "        attention will take place. May be a set, list, or tuple.\n",
    "        For example, if this contains 4, then at 4x downsampling, attention\n",
    "        will be used.\n",
    "    :param dropout: the dropout probability.\n",
    "    :param channel_mult: channel multiplier for each level of the UNet.\n",
    "    :param conv_resample: if True, use learned convolutions for upsampling and\n",
    "        downsampling.\n",
    "    :param dims: determines if the signal is 1D, 2D, or 3D.\n",
    "    :param num_classes: if specified (as an int), then this model will be\n",
    "        class-conditional with `num_classes` classes.\n",
    "    :param use_checkpoint: use gradient checkpointing to reduce memory usage.\n",
    "    :param num_heads: the number of attention heads in each attention layer.\n",
    "    :param num_heads_channels: if specified, ignore num_heads and instead use\n",
    "                               a fixed channel width per attention head.\n",
    "    :param num_heads_upsample: works with num_heads to set a different number\n",
    "                               of heads for upsampling. Deprecated.\n",
    "    :param use_scale_shift_norm: use a FiLM-like conditioning mechanism.\n",
    "    :param resblock_updown: use residual blocks for up/downsampling.\n",
    "    :param use_new_attention_order: use a different attention pattern for potentially\n",
    "                                    increased efficiency.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size,\n",
    "        in_channels,\n",
    "        model_channels,\n",
    "        out_channels,\n",
    "        num_res_blocks,\n",
    "        attention_resolutions,\n",
    "        dropout=0,\n",
    "        channel_mult=(1, 2, 4, 8),\n",
    "        conv_resample=True,\n",
    "        dims=2,\n",
    "        num_classes=None,\n",
    "        use_checkpoint=False,\n",
    "        use_fp16=False,\n",
    "        num_heads=1,\n",
    "        num_head_channels=-1,\n",
    "        num_heads_upsample=-1,\n",
    "        use_scale_shift_norm=False,\n",
    "        resblock_updown=False,\n",
    "        use_new_attention_order=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if num_heads_upsample == -1:\n",
    "            num_heads_upsample = num_heads\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.in_channels = in_channels\n",
    "        self.model_channels = model_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        self.attention_resolutions = attention_resolutions\n",
    "        self.dropout = dropout\n",
    "        self.channel_mult = channel_mult\n",
    "        self.conv_resample = conv_resample\n",
    "        self.num_classes = num_classes\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        self.dtype = th.float16 if use_fp16 else th.float32\n",
    "        self.num_heads = num_heads\n",
    "        self.num_head_channels = num_head_channels\n",
    "        self.num_heads_upsample = num_heads_upsample\n",
    "\n",
    "        time_embed_dim = model_channels * 4\n",
    "        self.time_embed = nn.Sequential(\n",
    "            linear(model_channels, time_embed_dim),\n",
    "            nn.SiLU(),\n",
    "            linear(time_embed_dim, time_embed_dim),\n",
    "        )\n",
    "\n",
    "        if self.num_classes is not None:\n",
    "            self.label_emb = nn.Embedding(num_classes, time_embed_dim)\n",
    "\n",
    "        ch = input_ch = int(channel_mult[0] * model_channels)\n",
    "        self.input_blocks = nn.ModuleList(\n",
    "            [TimestepEmbedSequential(conv_nd(dims, in_channels, ch, 3, padding=1))]\n",
    "        )\n",
    "        self._feature_size = ch\n",
    "        input_block_chans = [ch]\n",
    "        ds = 1\n",
    "        for level, mult in enumerate(channel_mult):\n",
    "            for _ in range(num_res_blocks):\n",
    "                layers = [\n",
    "                    ResBlock(\n",
    "                        ch,\n",
    "                        time_embed_dim,\n",
    "                        dropout,\n",
    "                        out_channels=int(mult * model_channels),\n",
    "                        dims=dims,\n",
    "                        use_checkpoint=use_checkpoint,\n",
    "                        use_scale_shift_norm=use_scale_shift_norm,\n",
    "                    )\n",
    "                ]\n",
    "                ch = int(mult * model_channels)\n",
    "                if ds in attention_resolutions:\n",
    "                    layers.append(\n",
    "                        AttentionBlock(\n",
    "                            ch,\n",
    "                            use_checkpoint=use_checkpoint,\n",
    "                            num_heads=num_heads,\n",
    "                            num_head_channels=num_head_channels,\n",
    "                            use_new_attention_order=use_new_attention_order,\n",
    "                        )\n",
    "                    )\n",
    "                self.input_blocks.append(TimestepEmbedSequential(*layers))\n",
    "                self._feature_size += ch\n",
    "                input_block_chans.append(ch)\n",
    "            if level != len(channel_mult) - 1:\n",
    "                out_ch = ch\n",
    "                self.input_blocks.append(\n",
    "                    TimestepEmbedSequential(\n",
    "                        ResBlock(\n",
    "                            ch,\n",
    "                            time_embed_dim,\n",
    "                            dropout,\n",
    "                            out_channels=out_ch,\n",
    "                            dims=dims,\n",
    "                            use_checkpoint=use_checkpoint,\n",
    "                            use_scale_shift_norm=use_scale_shift_norm,\n",
    "                            down=True,\n",
    "                        )\n",
    "                        if resblock_updown\n",
    "                        else Downsample(ch, conv_resample, dims=dims, out_channels=out_ch)\n",
    "                    )\n",
    "                )\n",
    "                ch = out_ch\n",
    "                input_block_chans.append(ch)\n",
    "                ds *= 2\n",
    "                self._feature_size += ch\n",
    "\n",
    "        self.middle_block = TimestepEmbedSequential(\n",
    "            ResBlock(\n",
    "                ch,\n",
    "                time_embed_dim,\n",
    "                dropout,\n",
    "                dims=dims,\n",
    "                use_checkpoint=use_checkpoint,\n",
    "                use_scale_shift_norm=use_scale_shift_norm,\n",
    "            ),\n",
    "            AttentionBlock(\n",
    "                ch,\n",
    "                use_checkpoint=use_checkpoint,\n",
    "                num_heads=num_heads,\n",
    "                num_head_channels=num_head_channels,\n",
    "                use_new_attention_order=use_new_attention_order,\n",
    "            ),\n",
    "            ResBlock(\n",
    "                ch,\n",
    "                time_embed_dim,\n",
    "                dropout,\n",
    "                dims=dims,\n",
    "                use_checkpoint=use_checkpoint,\n",
    "                use_scale_shift_norm=use_scale_shift_norm,\n",
    "            ),\n",
    "        )\n",
    "        self._feature_size += ch\n",
    "\n",
    "        self.output_blocks = nn.ModuleList([])\n",
    "        for level, mult in list(enumerate(channel_mult))[::-1]:\n",
    "            for i in range(num_res_blocks + 1):\n",
    "                ich = input_block_chans.pop()\n",
    "                layers = [\n",
    "                    ResBlock(\n",
    "                        ch + ich,\n",
    "                        time_embed_dim,\n",
    "                        dropout,\n",
    "                        out_channels=int(model_channels * mult),\n",
    "                        dims=dims,\n",
    "                        use_checkpoint=use_checkpoint,\n",
    "                        use_scale_shift_norm=use_scale_shift_norm,\n",
    "                    )\n",
    "                ]\n",
    "                ch = int(model_channels * mult)\n",
    "                if ds in attention_resolutions:\n",
    "                    layers.append(\n",
    "                        AttentionBlock(\n",
    "                            ch,\n",
    "                            use_checkpoint=use_checkpoint,\n",
    "                            num_heads=num_heads_upsample,\n",
    "                            num_head_channels=num_head_channels,\n",
    "                            use_new_attention_order=use_new_attention_order,\n",
    "                        )\n",
    "                    )\n",
    "                if level and i == num_res_blocks:\n",
    "                    out_ch = ch\n",
    "                    layers.append(\n",
    "                        ResBlock(\n",
    "                            ch,\n",
    "                            time_embed_dim,\n",
    "                            dropout,\n",
    "                            out_channels=out_ch,\n",
    "                            dims=dims,\n",
    "                            use_checkpoint=use_checkpoint,\n",
    "                            use_scale_shift_norm=use_scale_shift_norm,\n",
    "                            up=True,\n",
    "                        )\n",
    "                        if resblock_updown\n",
    "                        else Upsample(ch, conv_resample, dims=dims, out_channels=out_ch)\n",
    "                    )\n",
    "                    ds //= 2\n",
    "                self.output_blocks.append(TimestepEmbedSequential(*layers))\n",
    "                self._feature_size += ch\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            normalization(ch),\n",
    "            nn.SiLU(),\n",
    "            zero_module(conv_nd(dims, input_ch, out_channels, 3, padding=1)),\n",
    "        )\n",
    "\n",
    "    def convert_to_fp16(self):\n",
    "        \"\"\"Convert the torso of the model to float16.\"\"\"\n",
    "        self.input_blocks.apply(convert_module_to_f16)\n",
    "        self.middle_block.apply(convert_module_to_f16)\n",
    "        self.output_blocks.apply(convert_module_to_f16)\n",
    "\n",
    "    def convert_to_fp32(self):\n",
    "        \"\"\"Convert the torso of the model to float32.\"\"\"\n",
    "        self.input_blocks.apply(convert_module_to_f32)\n",
    "        self.middle_block.apply(convert_module_to_f32)\n",
    "        self.output_blocks.apply(convert_module_to_f32)\n",
    "\n",
    "    def forward(self, inputs, y=None):\n",
    "        \"\"\"Apply the model to an input batch.\n",
    "\n",
    "        :param x: an [N x C x ...] Tensor of inputs.\n",
    "        :param timesteps: a 1-D batch of timesteps.\n",
    "        :param y: an [N] Tensor of labels, if class-conditional.\n",
    "        :return: an [N x C x ...] Tensor of outputs.\n",
    "        \"\"\"\n",
    "        x = inputs[:,1:].reshape((-1, 1, 28, 28))\n",
    "        timesteps = inputs[:,0]\n",
    "        \n",
    "        assert (y is not None) == (\n",
    "            self.num_classes is not None\n",
    "        ), \"must specify y if and only if the model is class-conditional\"\n",
    "        while timesteps.dim() > 1:\n",
    "            #print(timesteps.shape)\n",
    "            timesteps = timesteps[:, 0]\n",
    "        if timesteps.dim() == 0:\n",
    "            timesteps = timesteps.repeat(x.shape[0])\n",
    "\n",
    "        hs = []\n",
    "        emb = self.time_embed(timestep_embedding(timesteps, self.model_channels))\n",
    "\n",
    "        if self.num_classes is not None:\n",
    "            assert y.shape == (x.shape[0],)\n",
    "            emb = emb + self.label_emb(y)\n",
    "\n",
    "        h = x.type(self.dtype)\n",
    "        for module in self.input_blocks:\n",
    "            h = module(h, emb)\n",
    "            hs.append(h)\n",
    "        h = self.middle_block(h, emb)\n",
    "        for module in self.output_blocks:\n",
    "            h = th.cat([h, hs.pop()], dim=1)\n",
    "            h = module(h, emb)\n",
    "        h = h.type(x.dtype)\n",
    "        h = self.out(h)\n",
    "        #h = h.reshape(inputs[:,1:].shape)\n",
    "        #print(h)\n",
    "        return torch.hstack((inputs, h))\n",
    "\n",
    "\n",
    "class UNetModelWrapper_encoder(UNetModel_encoder):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_channels,\n",
    "        num_res_blocks,\n",
    "        channel_mult=None,\n",
    "        learn_sigma=False,\n",
    "        class_cond=False,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        use_checkpoint=False,\n",
    "        attention_resolutions=\"16\",\n",
    "        num_heads=1,\n",
    "        num_head_channels=-1,\n",
    "        num_heads_upsample=-1,\n",
    "        use_scale_shift_norm=False,\n",
    "        dropout=0,\n",
    "        resblock_updown=False,\n",
    "        use_fp16=False,\n",
    "        use_new_attention_order=False,\n",
    "    ):\n",
    "        \"\"\"Dim (tuple): (C, H, W)\"\"\"\n",
    "        image_size = dim[-1]\n",
    "        if channel_mult is None:\n",
    "            if image_size == 512:\n",
    "                channel_mult = (0.5, 1, 1, 2, 2, 4, 4)\n",
    "            elif image_size == 256:\n",
    "                channel_mult = (1, 1, 2, 2, 4, 4)\n",
    "            elif image_size == 128:\n",
    "                channel_mult = (1, 1, 2, 3, 4)\n",
    "            elif image_size == 64:\n",
    "                channel_mult = (1, 2, 3, 4)\n",
    "            elif image_size == 32:\n",
    "                channel_mult = (1, 2, 2, 2)\n",
    "            elif image_size == 28:\n",
    "                channel_mult = (1, 2, 2)\n",
    "            else:\n",
    "                raise ValueError(f\"unsupported image size: {image_size}\")\n",
    "        else:\n",
    "            channel_mult = list(channel_mult)\n",
    "\n",
    "        attention_ds = []\n",
    "        for res in attention_resolutions.split(\",\"):\n",
    "            attention_ds.append(image_size // int(res))\n",
    "\n",
    "        return super().__init__(\n",
    "            image_size=image_size,\n",
    "            in_channels=dim[0],\n",
    "            model_channels=num_channels,\n",
    "            out_channels=(dim[0] if not learn_sigma else dim[0] * 2),\n",
    "            num_res_blocks=num_res_blocks,\n",
    "            attention_resolutions=tuple(attention_ds),\n",
    "            dropout=dropout,\n",
    "            channel_mult=channel_mult,\n",
    "            num_classes=(num_classes if class_cond else None),\n",
    "            use_checkpoint=use_checkpoint,\n",
    "            use_fp16=use_fp16,\n",
    "            num_heads=num_heads,\n",
    "            num_head_channels=num_head_channels,\n",
    "            num_heads_upsample=num_heads_upsample,\n",
    "            use_scale_shift_norm=use_scale_shift_norm,\n",
    "            resblock_updown=resblock_updown,\n",
    "            use_new_attention_order=use_new_attention_order,\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, y=None, *args, **kwargs):\n",
    "        return super().forward(inputs, y=y)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dim=(1, 28, 28)):\n",
    "        super().__init__()\n",
    "        self.input_dimension = dim[-1]*dim[-2]+1\n",
    "    \n",
    "    def forward(self, tensor2d_x):\n",
    "        return tensor2d_x[:,:self.input_dimension]\n",
    "\n",
    "class Autoencoder_unet(nn.Module):\n",
    "    def __init__(self, num_channels,num_res_blocks, dim):\n",
    "        super().__init__()\n",
    "        self.encoder = UNetModelWrapper_encoder(dim=dim, num_channels=num_channels, num_res_blocks=num_res_blocks).to(device)\n",
    "        self.decoder = Decoder(dim)\n",
    "        \n",
    "\n",
    "    def forward(self, tensor2d_x: torch.Tensor):\n",
    "        tensor2d_x = self.encoder(tensor2d_x)\n",
    "        return self.decoder(tensor2d_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchmetrics[image] in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy>1.20.0 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torchmetrics[image]) (2.2.5)\n",
      "Requirement already satisfied: packaging>17.1 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torchmetrics[image]) (24.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torchmetrics[image]) (2.7.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torchmetrics[image]) (0.14.3)\n",
      "Requirement already satisfied: torch-fidelity<=0.4.0 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torchmetrics[image]) (0.3.0)\n",
      "Requirement already satisfied: torchvision>=0.15.1 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torchmetrics[image]) (0.22.0)\n",
      "Requirement already satisfied: scipy>1.0.0 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torchmetrics[image]) (1.15.3)\n",
      "Requirement already satisfied: Pillow in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch-fidelity<=0.4.0->torchmetrics[image]) (11.2.1)\n",
      "Requirement already satisfied: tqdm in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch-fidelity<=0.4.0->torchmetrics[image]) (4.67.1)\n",
      "Requirement already satisfied: setuptools in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics[image]) (78.1.1)\n",
      "Requirement already satisfied: typing_extensions in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics[image]) (4.13.2)\n",
      "Requirement already satisfied: filelock in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics[image]) (3.18.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics[image]) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics[image]) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics[image]) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics[image]) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics[image]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics[image]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics[image]) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics[image]) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics[image]) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics[image]) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics[image]) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics[image]) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics[image]) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics[image]) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics[image]) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics[image]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics[image]) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics[image]) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics[image]) (3.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics[image]) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->torchmetrics[image]) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchmetrics[image]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch-fidelity in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (0.3.0)\n",
      "Requirement already satisfied: numpy in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch-fidelity) (2.2.5)\n",
      "Requirement already satisfied: Pillow in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch-fidelity) (11.2.1)\n",
      "Requirement already satisfied: scipy in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch-fidelity) (1.15.3)\n",
      "Requirement already satisfied: torch in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch-fidelity) (2.7.0)\n",
      "Requirement already satisfied: torchvision in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch-fidelity) (0.22.0)\n",
      "Requirement already satisfied: tqdm in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch-fidelity) (4.67.1)\n",
      "Requirement already satisfied: filelock in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch->torch-fidelity) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch->torch-fidelity) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch->torch-fidelity) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch->torch-fidelity) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch->torch-fidelity) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch->torch-fidelity) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch->torch-fidelity) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch->torch-fidelity) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch->torch-fidelity) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch->torch-fidelity) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch->torch-fidelity) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch->torch-fidelity) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch->torch-fidelity) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch->torch-fidelity) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch->torch-fidelity) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch->torch-fidelity) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch->torch-fidelity) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch->torch-fidelity) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch->torch-fidelity) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch->torch-fidelity) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from torch->torch-fidelity) (3.3.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from triton==3.3.0->torch->torch-fidelity) (78.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from sympy>=1.13.3->torch->torch-fidelity) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/turan/miniconda3/envs/torchcfm/lib/python3.10/site-packages (from jinja2->torch->torch-fidelity) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch-fidelity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation friendly code for ablations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: FID_AVAILABLE = True\n",
      "Using device: cuda\n",
      "Koopman-CFM Training with Loss and Embedding Dimension Ablation\n",
      "================================================================================\n",
      "KOOPMAN-CFM TRAINING CONFIGURATION\n",
      "================================================================================\n",
      "  Consistency Loss: True\n",
      "  Target Loss: True\n",
      "  Target Phase Loss: True\n",
      "  Embedding Dimension: 1765\n",
      "Loaded target FID from baseline: 28.92\n",
      "  Target FID: 28.92\n",
      "================================================================================\n",
      "No saved pairs found at cfm_pairs.pth\n",
      "Generating new CFM pairs...\n",
      "Generating 100000 CFM pairs using trajectory method...\n",
      "Generating batch: 1 to 200 of 100000\n",
      "Trajectory sampling time: 11.36s\n",
      "Generating batch: 201 to 400 of 100000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 857\u001b[0m\n\u001b[1;32m    851\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKoopman-CFM Training with Loss and Embedding Dimension Ablation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 857\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_koopman_cfm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Loss ablation\u001b[39;49;00m\n\u001b[1;32m    859\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconsistency_loss_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_loss_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_phase_loss_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Dimension ablation  \u001b[39;49;00m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1765\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Training settings\u001b[39;49;00m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#Just to see if everything was working \u001b[39;49;00m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3e-4\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#Learning rate used by Omri\u001b[39;49;00m\n\u001b[1;32m    870\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;66;03m# Test the trained model (will automatically find the right checkpoint)\u001b[39;00m\n\u001b[1;32m    873\u001b[0m test_sampling(embedding_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1765\u001b[39m)\n",
      "Cell \u001b[0;32mIn[62], line 513\u001b[0m, in \u001b[0;36mtrain_koopman_cfm\u001b[0;34m(consistency_loss_on, target_loss_on, target_phase_loss_on, consistency_weight, target_weight, target_phase_weight, embedding_dim, n_epochs, batch_size, lr, fid_n_samples, n_cfm_pairs, pairs_batch_size)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x0_pairs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating new CFM pairs...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 513\u001b[0m     x0_pairs, x1_pairs \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_cfm_pairs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfm_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_cfm_pairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpairs_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    515\u001b[0m pairs_dataset \u001b[38;5;241m=\u001b[39m CFMPairsDataset(x0_pairs, x1_pairs)\n\u001b[1;32m    516\u001b[0m pairs_loader \u001b[38;5;241m=\u001b[39m DataLoader(pairs_dataset, batch_size\u001b[38;5;241m=\u001b[39mpairs_batch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[62], line 111\u001b[0m, in \u001b[0;36mgenerate_cfm_pairs\u001b[0;34m(cfm_model, n_pairs, batch_size, save_path)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# Generate trajectory from t=0 to t=1 using the underlying CFM\u001b[39;00m\n\u001b[1;32m    110\u001b[0m top \u001b[38;5;241m=\u001b[39m time()\n\u001b[0;32m--> 111\u001b[0m traj \u001b[38;5;241m=\u001b[39m \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrajectory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx0_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mt_span\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinspace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrajectory sampling time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(time()\u001b[38;5;241m-\u001b[39mtop)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Extract x0 (initial) and x1 (final) states\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torchcfm/lib/python3.10/site-packages/torchdyn/core/neuralde.py:100\u001b[0m, in \u001b[0;36mNeuralODE.trajectory\u001b[0;34m(self, x, t_span)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrajectory\u001b[39m(\u001b[38;5;28mself\u001b[39m, x:torch\u001b[38;5;241m.\u001b[39mTensor, t_span:Tensor):\n\u001b[1;32m     99\u001b[0m     x, t_span \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prep_integration(x, t_span)\n\u001b[0;32m--> 100\u001b[0m     _, sol \u001b[38;5;241m=\u001b[39m \u001b[43modeint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_span\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrtol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sol\n",
      "File \u001b[0;32m~/miniconda3/envs/torchcfm/lib/python3.10/site-packages/torchdyn/numerics/odeint.py:91\u001b[0m, in \u001b[0;36modeint\u001b[0;34m(f, x, t_span, solver, atol, rtol, t_stops, verbose, interpolator, return_all_eval, save_at, args, seminorm)\u001b[0m\n\u001b[1;32m     89\u001b[0m dt \u001b[38;5;241m=\u001b[39m init_step(f, k1, x, t, solver\u001b[38;5;241m.\u001b[39morder, atol, rtol)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(save_at) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m: warn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSetting save_at has no effect on adaptive-step methods\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_adaptive_odeint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_span\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_all_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseminorm\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torchcfm/lib/python3.10/site-packages/torchdyn/numerics/odeint.py:363\u001b[0m, in \u001b[0;36m_adaptive_odeint\u001b[0;34m(f, k1, x, dt, t_span, solver, atol, rtol, args, interpolator, return_all_eval, seminorm)\u001b[0m\n\u001b[1;32m    360\u001b[0m \t\t\tdt_old, ckpt_flag \u001b[38;5;241m=\u001b[39m dt, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \t\t\tdt \u001b[38;5;241m=\u001b[39m t_eval[ckpt_counter] \u001b[38;5;241m-\u001b[39m t\n\u001b[0;32m--> 363\u001b[0m f_new, x_new, x_err, stages \u001b[38;5;241m=\u001b[39m \u001b[43msolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;66;03m################# compute error #############################\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m seminorm[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/torchcfm/lib/python3.10/site-packages/torchdyn/numerics/solvers/ode.py:153\u001b[0m, in \u001b[0;36mDormandPrince45.step\u001b[0;34m(self, f, x, t, dt, k1, args)\u001b[0m\n\u001b[1;32m    151\u001b[0m k5 \u001b[38;5;241m=\u001b[39m f(t \u001b[38;5;241m+\u001b[39m c[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m*\u001b[39m dt, x \u001b[38;5;241m+\u001b[39m dt \u001b[38;5;241m*\u001b[39m a[\u001b[38;5;241m3\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m k1 \u001b[38;5;241m+\u001b[39m dt \u001b[38;5;241m*\u001b[39m a[\u001b[38;5;241m3\u001b[39m][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m k2 \u001b[38;5;241m+\u001b[39m dt \u001b[38;5;241m*\u001b[39m a[\u001b[38;5;241m3\u001b[39m][\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m*\u001b[39m k3 \u001b[38;5;241m+\u001b[39m dt \u001b[38;5;241m*\u001b[39m a[\u001b[38;5;241m3\u001b[39m][\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m*\u001b[39m k4)\n\u001b[1;32m    152\u001b[0m k6 \u001b[38;5;241m=\u001b[39m f(t \u001b[38;5;241m+\u001b[39m c[\u001b[38;5;241m4\u001b[39m] \u001b[38;5;241m*\u001b[39m dt, x \u001b[38;5;241m+\u001b[39m dt \u001b[38;5;241m*\u001b[39m a[\u001b[38;5;241m4\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m k1 \u001b[38;5;241m+\u001b[39m dt \u001b[38;5;241m*\u001b[39m a[\u001b[38;5;241m4\u001b[39m][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m k2 \u001b[38;5;241m+\u001b[39m dt \u001b[38;5;241m*\u001b[39m a[\u001b[38;5;241m4\u001b[39m][\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m*\u001b[39m k3 \u001b[38;5;241m+\u001b[39m dt \u001b[38;5;241m*\u001b[39m a[\u001b[38;5;241m4\u001b[39m][\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m*\u001b[39m k4 \u001b[38;5;241m+\u001b[39m dt \u001b[38;5;241m*\u001b[39m a[\u001b[38;5;241m4\u001b[39m][\u001b[38;5;241m4\u001b[39m] \u001b[38;5;241m*\u001b[39m k5)\n\u001b[0;32m--> 153\u001b[0m k7 \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk3\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk4\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk5\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk6\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m x_sol \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m dt \u001b[38;5;241m*\u001b[39m (bsol[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m k1 \u001b[38;5;241m+\u001b[39m bsol[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m k2 \u001b[38;5;241m+\u001b[39m bsol[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m*\u001b[39m k3 \u001b[38;5;241m+\u001b[39m bsol[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m*\u001b[39m k4 \u001b[38;5;241m+\u001b[39m bsol[\u001b[38;5;241m4\u001b[39m] \u001b[38;5;241m*\u001b[39m k5 \u001b[38;5;241m+\u001b[39m bsol[\u001b[38;5;241m5\u001b[39m] \u001b[38;5;241m*\u001b[39m k6)\n\u001b[1;32m    155\u001b[0m err \u001b[38;5;241m=\u001b[39m dt \u001b[38;5;241m*\u001b[39m (berr[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m k1 \u001b[38;5;241m+\u001b[39m berr[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m k2 \u001b[38;5;241m+\u001b[39m berr[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m*\u001b[39m k3 \u001b[38;5;241m+\u001b[39m berr[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m*\u001b[39m k4 \u001b[38;5;241m+\u001b[39m berr[\u001b[38;5;241m4\u001b[39m] \u001b[38;5;241m*\u001b[39m k5 \u001b[38;5;241m+\u001b[39m berr[\u001b[38;5;241m5\u001b[39m] \u001b[38;5;241m*\u001b[39m k6 \u001b[38;5;241m+\u001b[39m berr[\u001b[38;5;241m6\u001b[39m] \u001b[38;5;241m*\u001b[39m k7)\n",
      "File \u001b[0;32m~/miniconda3/envs/torchcfm/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1744\u001b[0m             tracing_state\u001b[38;5;241m.\u001b[39mpop_scope()\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m-> 1747\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_wrapped_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torchcfm.conditional_flow_matching import ExactOptimalTransportConditionalFlowMatcher\n",
    "from torchcfm.models.unet import UNetModel\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import sys\n",
    "from contextlib import redirect_stdout\n",
    "import io\n",
    "import traceback\n",
    "from time import time\n",
    "import glob\n",
    "\n",
    "# FID computation imports\n",
    "try:\n",
    "    from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "    FID_AVAILABLE = True\n",
    "    print(\"DEBUG: FID_AVAILABLE = True\")\n",
    "except ImportError as e:\n",
    "    print(f\"Warning: torchmetrics import failed: {e}\")\n",
    "    FID_AVAILABLE = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load pre-trained CFM model, it's trained further above \n",
    "def load_pretrained_cfm():\n",
    "    cfm_model = UNetModel(dim=(1, 28, 28), num_channels=32, num_res_blocks=1).to(device)\n",
    "    cfm_model.load_state_dict(torch.load(\"/home/turan/koopman/cfm_model/unet_model_mnist.pth\"))\n",
    "    cfm_model.eval()\n",
    "    return cfm_model\n",
    "\n",
    "\n",
    "# Function to get real samples from baseline\n",
    "def get_real_samples_from_baseline(baseline_path='cfm_baseline_results.pth', n_samples=1000):\n",
    "    \"\"\"Load real samples from the baseline computation\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(baseline_path):\n",
    "            print(f\"Loading real samples from baseline file: {baseline_path}\")\n",
    "            baseline_data = torch.load(baseline_path)\n",
    "            real_samples = baseline_data['real_samples'][:n_samples]\n",
    "            print(f\"Loaded {real_samples.shape[0]} real samples from baseline\")\n",
    "            return real_samples\n",
    "        else:\n",
    "            print(f\"Baseline file not found: {baseline_path}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading baseline samples: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_latest_checkpoint(embedding_dim, max_epoch=None):\n",
    "    \"\"\"Find the latest checkpoint for a given embedding dimension\"\"\"\n",
    "    # Look for checkpoints with this embedding dimension\n",
    "    pattern = f\"koopman_model_dim_{embedding_dim}_epoch_*.pth\"\n",
    "    checkpoints = glob.glob(pattern)\n",
    "    \n",
    "    if not checkpoints:\n",
    "        return None\n",
    "    \n",
    "    # Extract epoch numbers and find the latest\n",
    "    epochs = []\n",
    "    for checkpoint in checkpoints:\n",
    "        try:\n",
    "            epoch = int(checkpoint.split('_epoch_')[1].split('.pth')[0])\n",
    "            if max_epoch is None or epoch <= max_epoch:\n",
    "                epochs.append((epoch, checkpoint))\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if not epochs:\n",
    "        return None\n",
    "    \n",
    "    # Return the checkpoint with highest epoch number\n",
    "    epochs.sort(key=lambda x: x[0])\n",
    "    return epochs[-1][1]\n",
    "\n",
    "# Function to generate and save x0, x1 pairs using CFM trajectories\n",
    "def generate_cfm_pairs(cfm_model, n_pairs=10000, batch_size=200, save_path='cfm_pairs.pth'):\n",
    "    \"\"\"Generate n_pairs of (x0, x1) using CFM trajectories and save them\"\"\"\n",
    "    from torchdyn.core import NeuralODE\n",
    "    \n",
    "    print(f\"Generating {n_pairs} CFM pairs using trajectory method...\")\n",
    "    \n",
    "    # Create NeuralODE for trajectory generation\n",
    "    node = NeuralODE(cfm_model, solver=\"dopri5\", sensitivity=\"adjoint\", atol=1e-4, rtol=1e-4)\n",
    "    \n",
    "    x0_list = []\n",
    "    x1_list = []\n",
    "    collected = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        while collected < n_pairs:\n",
    "            # Calculate how many samples to generate in this batch\n",
    "            samples_needed = min(batch_size, n_pairs - collected)\n",
    "            \n",
    "            print(f\"Generating batch: {collected + 1} to {collected + samples_needed} of {n_pairs}\")\n",
    "            \n",
    "            # Generate initial noise\n",
    "            x0_batch = torch.randn(samples_needed, 1, 28, 28, device=device)\n",
    "            \n",
    "            # Generate trajectory from t=0 to t=1 using the underlying CFM\n",
    "            top = time()\n",
    "            traj = node.trajectory(\n",
    "                x0_batch,\n",
    "                t_span=torch.linspace(0, 1, 100, device=device),\n",
    "            )\n",
    "            print(f'Trajectory sampling time: {(time()-top):.2f}s')\n",
    "            \n",
    "            # Extract x0 (initial) and x1 (final) states\n",
    "            x0_final = traj[0]  # t=0\n",
    "            x1_final = traj[-1]  # t=1\n",
    "            \n",
    "            x0_list.append(x0_final.cpu())\n",
    "            x1_list.append(x1_final.cpu())\n",
    "            \n",
    "            collected += samples_needed\n",
    "            \n",
    "            # Free GPU memory\n",
    "            del traj, x0_batch\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Concatenate all pairs\n",
    "    x0_all = torch.cat(x0_list, dim=0)\n",
    "    x1_all = torch.cat(x1_list, dim=0)\n",
    "    \n",
    "    # Save pairs\n",
    "    torch.save({\n",
    "        'x0': x0_all,\n",
    "        'x1': x1_all,\n",
    "        'n_pairs': n_pairs\n",
    "    }, save_path)\n",
    "    \n",
    "    print(f\"Saved {n_pairs} CFM pairs to {save_path}\")\n",
    "    print(f\"x0 shape: {x0_all.shape}, x1 shape: {x1_all.shape}\")\n",
    "    return x0_all, x1_all\n",
    "\n",
    "# Function to load saved pairs\n",
    "def load_cfm_pairs(load_path='cfm_pairs.pth'):\n",
    "    \"\"\"Load saved CFM pairs\"\"\"\n",
    "    if os.path.exists(load_path):\n",
    "        data = torch.load(load_path)\n",
    "        print(f\"Loaded {data['n_pairs']} CFM pairs from {load_path}\")\n",
    "        return data['x0'], data['x1']\n",
    "    else:\n",
    "        print(f\"No saved pairs found at {load_path}\")\n",
    "        return None, None\n",
    "\n",
    "# Dataset class for CFM pairs\n",
    "class CFMPairsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x0, x1):\n",
    "        self.x0 = x0\n",
    "        self.x1 = x1\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x0)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x0[idx], self.x1[idx]\n",
    "\n",
    "# Decoder - extracts image part from [t, x, W*g(x)]\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dim=(1, 28, 28)):\n",
    "        super().__init__()\n",
    "        self.input_dimension = dim[-1]*dim[-2]+1  # 784 + 1 for time\n",
    "        \n",
    "    def forward(self, tensor2d_x):\n",
    "        return tensor2d_x[:, :self.input_dimension]\n",
    "\n",
    "class ConcatenatingUNetEncoder(nn.Module):\n",
    "    def __init__(self, num_channels=32, num_res_blocks=1, dim=(1, 28, 28), embedding_dim=1569):\n",
    "        super().__init__()\n",
    "        self.base_unet = UNetModel(dim=dim, num_channels=num_channels, num_res_blocks=num_res_blocks)\n",
    "        self.input_dim = dim[0] * dim[1] * dim[2] + 1  # 784 + 1 for time\n",
    "        \n",
    "        # Calculate UNet output dimension\n",
    "        unet_output_dim = dim[0] * dim[1] * dim[2]  # 784 for MNIST\n",
    "        \n",
    "        # Learnable projection for UNet features only so that we can control the embedding dimension\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Validate embedding dimension, must be higher than the input data dimension :) \n",
    "        if embedding_dim < self.input_dim:\n",
    "            raise ValueError(f\"embedding_dim ({embedding_dim}) must be >= input_dim ({self.input_dim})\")\n",
    "        \n",
    "        # Only create projection if we need UNet features\n",
    "        self.projected_unet_dim = embedding_dim - self.input_dim\n",
    "        if self.projected_unet_dim > 0:\n",
    "            self.unet_projection = nn.Linear(unet_output_dim, self.projected_unet_dim)\n",
    "            # Initialize projection weights\n",
    "            nn.init.xavier_uniform_(self.unet_projection.weight)\n",
    "            nn.init.zeros_(self.unet_projection.bias)\n",
    "        else:\n",
    "            self.unet_projection = None\n",
    "        \n",
    "    def forward(self, t, x):\n",
    "        # Keep input unchanged: [t, x]\n",
    "        t_flat = t.expand(x.shape[0], 1) if t.dim() == 1 else t\n",
    "        x_flat = x.reshape(x.shape[0], -1)\n",
    "        input_concat = torch.cat([t_flat, x_flat], dim=1)  # [t, x] - unchanged\n",
    "        \n",
    "        if self.unet_projection is not None:\n",
    "            # I had weird prints coming out, this suppresses them. \n",
    "            with redirect_stdout(io.StringIO()):\n",
    "                unet_out = self.base_unet(t, x)\n",
    "            \n",
    "            # Project UNet features: g(x) -> W*g(x)\n",
    "            unet_flat = unet_out.reshape(unet_out.shape[0], -1)\n",
    "            projected_unet = self.unet_projection(unet_flat)  # W*g(x)\n",
    "            \n",
    "            # Final output: [t, x, W*g(x)]\n",
    "            embedded = torch.cat([input_concat, projected_unet], dim=1)\n",
    "        else:\n",
    "            # Only [t, x], no UNet features\n",
    "            embedded = input_concat\n",
    "        \n",
    "        return embedded\n",
    "\n",
    "# Autoencoder using concatenating UNet encoder with learnable projection\n",
    "class Autoencoder_unet(nn.Module):\n",
    "    def __init__(self, num_channels, num_res_blocks, dim, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = ConcatenatingUNetEncoder(num_channels, num_res_blocks, dim, embedding_dim).to(device)\n",
    "        self.decoder = Decoder(dim)\n",
    "        \n",
    "    def forward(self, tensor2d_x: torch.Tensor):\n",
    "        # Split input back to t and x for encoder\n",
    "        t = tensor2d_x[:, :1]\n",
    "        x = tensor2d_x[:, 1:].reshape(-1, 1, 28, 28)\n",
    "        encoded = self.encoder(t, x)\n",
    "        return self.decoder(encoded)\n",
    "\n",
    "# Koopman Generator (Lie operator)\n",
    "class KoopmanGenerator(nn.Module):\n",
    "    def __init__(self, operator_dim):\n",
    "        super().__init__()\n",
    "        self.operator_dim = operator_dim\n",
    "        self.L = nn.Parameter(torch.zeros(operator_dim, operator_dim))\n",
    "        nn.init.normal_(self.L, mean=0.0, std=1e-3)\n",
    "        \n",
    "    def forward(self, g_tilde):\n",
    "        return g_tilde @ self.L\n",
    "\n",
    "# Combined Koopman-CFM Model with controllable embedding dimension\n",
    "class KoopmanCFM(nn.Module):\n",
    "    def __init__(self, num_channels=32, num_res_blocks=1, dim=(1, 28, 28), embedding_dim=1569):\n",
    "        super().__init__()\n",
    "        self.autoencoder = Autoencoder_unet(num_channels, num_res_blocks, dim, embedding_dim)\n",
    "        self.generator = KoopmanGenerator(embedding_dim)  # Can control the embedding dimension of operator\n",
    "        self.input_dim = dim[-1] * dim[-2] + 1  # 784 + 1 for time\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "    def forward(self, x_t):\n",
    "        g_tilde = self.autoencoder.encoder(x_t)  # UNet concatenates and projects features\n",
    "        L_g = self.generator(g_tilde)\n",
    "        return g_tilde, L_g\n",
    "    \n",
    "    def decode(self, g_tilde):\n",
    "        # Use the decoder to extract image part\n",
    "        return self.autoencoder.decoder(g_tilde)\n",
    "\n",
    "def compute_fid(real_samples, generated_samples):\n",
    "    \"\"\"\n",
    "    Computes the Frechet Inception Distance (FID) between two sets of images.\n",
    "    \"\"\"\n",
    "    if not FID_AVAILABLE:\n",
    "        print(\"torchmetrics is not available. Skipping FID computation.\")\n",
    "        return float('nan') # Return Not a Number if library is missing\n",
    "\n",
    "    print(\"DEBUG: Initializing FID metric...\")\n",
    "    fid = FrechetInceptionDistance().to(device)\n",
    "\n",
    "    def prepare_samples(samples):\n",
    "        \"\"\"Prepares samples for FID computation: move to device, scale, and ensure 3 channels.\"\"\"\n",
    "        \n",
    "        samples = samples.to(device)\n",
    "        print(f\"DEBUG: Input samples range: [{samples.min():.3f}, {samples.max():.3f}]\")\n",
    "        \n",
    "        # Clamp, scale to [0, 255] and convert to uint8\n",
    "        \n",
    "        prepared = ((samples.clamp(-1, 1) + 1) / 2 * 255).to(torch.uint8)\n",
    "        \n",
    "        \n",
    "        if prepared.shape[1] == 1:\n",
    "            prepared = prepared.repeat(1, 3, 1, 1)\n",
    "            \n",
    "        print(f\"DEBUG: Prepared samples shape: {prepared.shape}, dtype: {prepared.dtype}\")\n",
    "        return prepared\n",
    "\n",
    "    try:\n",
    "        # Prepare both real and generated samples\n",
    "        real_prepared = prepare_samples(real_samples)\n",
    "        generated_prepared = prepare_samples(generated_samples)\n",
    "        \n",
    "        # Update the metric with batches of images\n",
    "        print(\"DEBUG: Updating FID with real samples...\")\n",
    "        fid.update(real_prepared, real=True)\n",
    "        print(\"DEBUG: Updating FID with generated samples...\")\n",
    "        fid.update(generated_prepared, real=False)\n",
    "        \n",
    "        # Compute and return the final score\n",
    "        return fid.compute().item()\n",
    "\n",
    "    except Exception as e:\n",
    "        # Catch potential errors during computation, like the device mismatch\n",
    "        print(f\"DEBUG: FID computation error: {e}\")\n",
    "        traceback.print_exc() # Print the full traceback for detailed debugging\n",
    "        return None # Return None on error\n",
    "\n",
    "# Function to compute target FID from CFM baseline\n",
    "def compute_target_fid():\n",
    "    \"\"\"Compute or load CFM baseline FID as target\"\"\"\n",
    "    baseline_path = 'cfm_baseline_results.pth'\n",
    "    \n",
    "    # Try to load existing baseline\n",
    "    if os.path.exists(baseline_path):\n",
    "        try:\n",
    "            baseline_data = torch.load(baseline_path)\n",
    "            target_fid = baseline_data.get('fid_score', None)\n",
    "            if target_fid is not None:\n",
    "                print(f\"Loaded target FID from baseline: {target_fid:.2f}\")\n",
    "                return target_fid\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading baseline: {e}\")\n",
    "    \n",
    "    # Compute baseline if not available\n",
    "    print(\"Computing CFM baseline FID...\")\n",
    "    try:\n",
    "        # Load CFM model\n",
    "        cfm_model = load_pretrained_cfm()\n",
    "        \n",
    "        # Sample from CFM using trajectories\n",
    "        from torchdyn.core import NeuralODE\n",
    "        node = NeuralODE(cfm_model, solver=\"dopri5\", sensitivity=\"adjoint\", atol=1e-4, rtol=1e-4)\n",
    "        \n",
    "        n_samples = 1000\n",
    "        batch_size = 100\n",
    "        all_samples = []\n",
    "        \n",
    "        print(\"Generating CFM samples for baseline FID...\")\n",
    "        with torch.no_grad():\n",
    "            for i in tqdm(range((n_samples + batch_size - 1) // batch_size), desc=\"CFM sampling\"):\n",
    "                current_batch_size = min(batch_size, n_samples - i * batch_size)\n",
    "                x0 = torch.randn(current_batch_size, 1, 28, 28, device=device)\n",
    "                \n",
    "                traj = node.trajectory(x0, t_span=torch.linspace(0, 1, 100, device=device))\n",
    "                all_samples.append(traj[-1].cpu()) \n",
    "                \n",
    "                del traj, x0\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        cfm_samples = torch.cat(all_samples, dim=0)[:n_samples]\n",
    "        \n",
    "        # Get real samples\n",
    "        print(\"Loading real MNIST samples...\")\n",
    "        trainset = datasets.MNIST(\"../data\", train=True, download=True,\n",
    "                                transform=transforms.Compose([\n",
    "                                    transforms.ToTensor(), \n",
    "                                    transforms.Normalize((0.5,), (0.5,))\n",
    "                                ]))\n",
    "        train_loader = DataLoader(trainset, batch_size=100, shuffle=True)\n",
    "        \n",
    "        real_samples = []\n",
    "        total_collected = 0\n",
    "        for batch, _ in train_loader:\n",
    "            real_samples.append(batch)\n",
    "            total_collected += batch.shape[0]\n",
    "            if total_collected >= n_samples:\n",
    "                break\n",
    "        \n",
    "        real_samples = torch.cat(real_samples, dim=0)[:n_samples]\n",
    "        \n",
    "        print(f\"DEBUG: Real samples shape: {real_samples.shape}, Generated samples shape: {cfm_samples.shape}\")\n",
    "\n",
    "        # Compute FID\n",
    "        if FID_AVAILABLE:\n",
    "            target_fid = compute_fid(real_samples, cfm_samples)\n",
    "            \n",
    "            if target_fid is None:\n",
    "                print(\"FID computation failed. Using fallback value.\")\n",
    "                return 27.75\n",
    "\n",
    "            # Save for future use\n",
    "            torch.save({\n",
    "                'real_samples': real_samples,\n",
    "                'cfm_samples': cfm_samples,\n",
    "                'fid_score': target_fid,\n",
    "                'n_samples': n_samples\n",
    "            }, baseline_path)\n",
    "            \n",
    "            print(f\"Computed and saved target FID: {target_fid:.2f}\")\n",
    "            return target_fid\n",
    "        else:\n",
    "            print(\"FID computation not available using default target\")\n",
    "            return 27.75\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error computing CFM baseline: {e}\")\n",
    "        traceback.print_exc() \n",
    "        return 27.75  # Fallback to a value I computed on the CFM model trained above\n",
    "\n",
    "\n",
    "# Sampling function\n",
    "def sample_koopman(model, n_samples=16, n_steps=100):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Initial noise\n",
    "        x0 = torch.randn(n_samples, 28, 28).to(device)\n",
    "        t0 = torch.zeros(n_samples, 1).to(device)\n",
    "        \n",
    "        # Encode initial state, encoder expects separate t and x arguments\n",
    "        g0 = model.autoencoder.encoder(t0, x0.reshape(n_samples, 1, 28, 28))\n",
    "        \n",
    "        # Matrix exponentiation for t=1\n",
    "        L_matrix = model.generator.L\n",
    "        exp_L = torch.matrix_exp(L_matrix)\n",
    "        \n",
    "        # Evolve in Koopman space\n",
    "        g1 = g0 @ exp_L\n",
    "        \n",
    "        # Decode to get final state\n",
    "        x1_decoded = model.decode(g1)\n",
    "        \n",
    "        # Extract images (removes the time dimension)\n",
    "        x1_images = x1_decoded[:, 1:].reshape(n_samples, 1, 28, 28)\n",
    "        \n",
    "        return x1_images\n",
    "\n",
    "# Training function with simple boolean ablation controls + embedding dimension ablation\n",
    "def train_koopman_cfm(\n",
    "    # Simple loss ablation flags\n",
    "    consistency_loss_on=True,\n",
    "    target_loss_on=True, \n",
    "    target_phase_loss_on=True,\n",
    "    \n",
    "    # Loss weights \n",
    "    consistency_weight=1.0,\n",
    "    target_weight=1.0,\n",
    "    target_phase_weight=1.0,\n",
    "    \n",
    "    # Embedding dimension ablation\n",
    "    embedding_dim=1569,  \n",
    "    # Standard training params\n",
    "    n_epochs=100,\n",
    "    batch_size=128,\n",
    "    lr=3e-4,\n",
    "    fid_n_samples=1000,\n",
    "    n_cfm_pairs=100000, #They use 250K-1M samples in Omri's paper \n",
    "    pairs_batch_size=64\n",
    "):\n",
    "    \"\"\"\n",
    "    Training function with embedding dimension ablation\n",
    "    \n",
    "    Args:\n",
    "        consistency_loss_on: Enable Koopman consistency loss\n",
    "        target_loss_on: Enable decoded endpoint matching loss\n",
    "        target_phase_loss_on: Enable phase space matching loss\n",
    "        *_weight: Loss weights (only used if loss is enabled)\n",
    "        embedding_dim: Koopman embedding dimension (ablation parameter)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Training configuration\n",
    "    print(\"=\" * 80)\n",
    "    print(\"KOOPMAN-CFM TRAINING CONFIGURATION\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"  Consistency Loss: {consistency_loss_on}\")\n",
    "    print(f\"  Target Loss: {target_loss_on}\")  \n",
    "    print(f\"  Target Phase Loss: {target_phase_loss_on}\")\n",
    "    print(f\"  Embedding Dimension: {embedding_dim}\")\n",
    "    \n",
    "    # Compute target FID\n",
    "    target_fid = compute_target_fid()\n",
    "    print(f\"  Target FID: {target_fid:.2f}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Validate at least one loss is on\n",
    "    if not any([consistency_loss_on, target_loss_on, target_phase_loss_on]):\n",
    "        raise ValueError(\"At least one loss must be enabled!\")\n",
    "    \n",
    "    # Validate embedding dimension\n",
    "    if embedding_dim < 785:\n",
    "        raise ValueError(\"Embedding dimension must be >= 785 (input dimension)\")\n",
    "    \n",
    "    # Load data\n",
    "    trainset = datasets.MNIST(\n",
    "        \"../data\", train=True, download=True,\n",
    "        transform=transforms.Compose([\n",
    "            transforms.ToTensor(), \n",
    "            transforms.Normalize((0.5,), (0.5,))\n",
    "        ])\n",
    "    )\n",
    "    train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    \n",
    "    # Load pre-trained CFM\n",
    "    cfm_model = load_pretrained_cfm()\n",
    "    FM = ExactOptimalTransportConditionalFlowMatcher(sigma=0.0)\n",
    "    \n",
    "    # Only load CFM pairs if target losses are enabled\n",
    "    pairs_loader = None\n",
    "    if target_loss_on or target_phase_loss_on:\n",
    "        pairs_path = 'cfm_pairs.pth'\n",
    "        x0_pairs, x1_pairs = load_cfm_pairs(pairs_path)\n",
    "        \n",
    "        if x0_pairs is None:\n",
    "            print(\"Generating new CFM pairs...\")\n",
    "            x0_pairs, x1_pairs = generate_cfm_pairs(cfm_model, n_cfm_pairs, batch_size=200, save_path=pairs_path)\n",
    "        \n",
    "        pairs_dataset = CFMPairsDataset(x0_pairs, x1_pairs)\n",
    "        pairs_loader = DataLoader(pairs_dataset, batch_size=pairs_batch_size, shuffle=True, drop_last=True)\n",
    "    \n",
    "    # Get real samples for FID\n",
    "    real_samples_fid = None\n",
    "    if FID_AVAILABLE:\n",
    "        real_samples_fid = get_real_samples_from_baseline('cfm_baseline_results.pth', fid_n_samples)\n",
    "        if real_samples_fid is not None:\n",
    "            real_samples_fid = real_samples_fid.to(device)\n",
    "    \n",
    "    # Initialize model with controllable embedding dimension\n",
    "    koopman_model = KoopmanCFM(\n",
    "        num_channels=32, \n",
    "        num_res_blocks=1, \n",
    "        dim=(1, 28, 28), \n",
    "        embedding_dim=embedding_dim  # Key parameter for the embedding dimension ablation\n",
    "    ).to(device)\n",
    "    \n",
    "    # Print model info (in Omri's paper they have plot of FID vs model capacity)\n",
    "    total_params = sum(p.numel() for p in koopman_model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in koopman_model.parameters() if p.requires_grad)\n",
    "    koopman_params = koopman_model.generator.L.numel()  # embedding_dim x embedding_dim\n",
    "    \n",
    "    print(f\"Model Parameters:\")\n",
    "    print(f\"  Total: {total_params:,}\")\n",
    "    print(f\"  Trainable: {trainable_params:,}\")\n",
    "    print(f\"  Koopman Operator (L): {koopman_params:,} ({embedding_dim}x{embedding_dim})\")\n",
    "    if embedding_dim > 785:\n",
    "        projection_params = 784 * (embedding_dim - 785) + (embedding_dim - 785)\n",
    "        print(f\"  Projection Parameters: {projection_params:,}\")\n",
    "    \n",
    "    optimizer = optim.Adam(koopman_model.parameters(), lr=lr)\n",
    "    #In omri's paper they use constant learning rate\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.8)\n",
    "    \n",
    "    fid_scores = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        koopman_model.train()\n",
    "        epoch_loss = 0\n",
    "        epoch_consistency = 0\n",
    "        epoch_target = 0  \n",
    "        epoch_target_phase = 0\n",
    "        \n",
    "        # Generate samples and compute FID\n",
    "        if epoch == 0 or epoch % 1 == 0:  # Report FID every epoch\n",
    "            samples = sample_koopman(koopman_model, n_samples=16)\n",
    "            \n",
    "            # Compute FID\n",
    "            fid_score = None\n",
    "            if FID_AVAILABLE and real_samples_fid is not None:\n",
    "                try:\n",
    "                    fid_samples = sample_koopman(koopman_model, n_samples=fid_n_samples)\n",
    "                    fid_score = compute_fid(real_samples_fid, fid_samples)\n",
    "                    if fid_score is not None:\n",
    "                        fid_scores.append(fid_score)\n",
    "                        print(f\"FID Score: {fid_score:.2f} (Target: {target_fid:.2f}) [Dim: {embedding_dim}]\")\n",
    "                except Exception as e:\n",
    "                    print(f\"FID failed: {e}\")\n",
    "            \n",
    "            # Plot samples\n",
    "            samples_display = torch.clamp(samples * 0.5 + 0.5, 0, 1)\n",
    "            grid = make_grid(samples_display, nrow=4, padding=2, normalize=False)\n",
    "            \n",
    "            plt.figure(figsize=(8, 8))\n",
    "            plt.imshow(grid.permute(1, 2, 0).cpu().numpy().squeeze(), cmap='gray')\n",
    "            plt.axis('off')\n",
    "            title = f'Epoch {epoch} - Embedding Dim: {embedding_dim}'\n",
    "            if fid_score is not None:\n",
    "                title += f'\\nFID: {fid_score:.2f}'\n",
    "            plt.title(title)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            os.makedirs(f'koopman_samples_dim_{embedding_dim}', exist_ok=True)\n",
    "            plt.savefig(f'koopman_samples_dim_{embedding_dim}/epoch_{epoch}.png', dpi=150, bbox_inches='tight')\n",
    "            plt.show()\n",
    "        \n",
    "        # Setup pairs iterator if needed\n",
    "        if pairs_loader is not None:\n",
    "            pairs_iter = iter(pairs_loader)\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{n_epochs} [Dim: {embedding_dim}]')\n",
    "        for batch_idx, (x1, _) in enumerate(pbar):\n",
    "            x1 = x1.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            total_loss = 0\n",
    "            consistency_loss = 0\n",
    "            target_loss = 0\n",
    "            target_phase_loss = 0\n",
    "            \n",
    "            # Consistency Loss\n",
    "            if consistency_loss_on:\n",
    "                x0 = torch.randn_like(x1)\n",
    "                t, xt, ut = FM.sample_location_and_conditional_flow(x0, x1) #Simulation free, we exploit the derived result (FM comes from the CFM)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    vt = cfm_model(t, xt)\n",
    "                \n",
    "                if t.dim() == 1:\n",
    "                    t = t.unsqueeze(1)\n",
    "                \n",
    "                g_tilde = koopman_model.autoencoder.encoder(t, xt)\n",
    "                L_g = koopman_model.generator(g_tilde)\n",
    "                \n",
    "                def encoder_func(t_input, x_input):\n",
    "                    return koopman_model.autoencoder.encoder(t_input, x_input)\n",
    "                \n",
    "                v_t_component = torch.ones_like(t)\n",
    "                v_x_component = vt\n",
    "                \n",
    "                jvp_result = autograd.functional.jvp(\n",
    "                    encoder_func, \n",
    "                    (t, xt), \n",
    "                    (v_t_component, v_x_component),\n",
    "                    create_graph=True\n",
    "                )[1]\n",
    "                \n",
    "                consistency_loss = nn.MSELoss()(L_g, jvp_result)\n",
    "                total_loss += consistency_weight * consistency_loss\n",
    "                epoch_consistency += consistency_loss.item()\n",
    "            \n",
    "            # Target Losses\n",
    "            if target_loss_on or target_phase_loss_on:\n",
    "                try:\n",
    "                    x0_pair, x1_pair = next(pairs_iter)\n",
    "                except StopIteration:\n",
    "                    pairs_iter = iter(pairs_loader)\n",
    "                    x0_pair, x1_pair = next(pairs_iter)\n",
    "                \n",
    "                x0_pair = x0_pair.to(device)\n",
    "                x1_pair = x1_pair.to(device)\n",
    "                \n",
    "                t_zero = torch.zeros(x0_pair.shape[0], 1).to(device)\n",
    "                t_one = torch.ones(x1_pair.shape[0], 1).to(device)\n",
    "                \n",
    "                g_x0 = koopman_model.autoencoder.encoder(t_zero, x0_pair)\n",
    "                g_x1 = koopman_model.autoencoder.encoder(t_one, x1_pair)\n",
    "                \n",
    "                L_exp = torch.matrix_exp(koopman_model.generator.L)\n",
    "                evolved_g_x0 = g_x0 @ L_exp\n",
    "                \n",
    "                if target_loss_on:\n",
    "                    decoded_evolved = koopman_model.decode(evolved_g_x0)\n",
    "                    decoded_target = koopman_model.decode(g_x1)\n",
    "                    target_loss = nn.MSELoss()(decoded_evolved, decoded_target)\n",
    "                    total_loss += target_weight * target_loss\n",
    "                    epoch_target += target_loss.item()\n",
    "                \n",
    "                if target_phase_loss_on:\n",
    "                    target_phase_loss = nn.MSELoss()(evolved_g_x0, g_x1)\n",
    "                    total_loss += target_phase_weight * target_phase_loss\n",
    "                    epoch_target_phase += target_phase_loss.item()\n",
    "            \n",
    "            # Backward pass\n",
    "            if total_loss > 0:\n",
    "                total_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(koopman_model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                epoch_loss += total_loss.item()\n",
    "            \n",
    "            # Progress bar\n",
    "            pbar_dict = {'Loss': f'{total_loss.item():.6f}', 'Dim': str(embedding_dim)}\n",
    "            if consistency_loss_on:\n",
    "                pbar_dict['Consist'] = f'{consistency_loss.item():.6f}'\n",
    "            if target_loss_on:\n",
    "                pbar_dict['Target'] = f'{target_loss.item():.6f}'\n",
    "            if target_phase_loss_on:\n",
    "                pbar_dict['Phase'] = f'{target_phase_loss.item():.6f}'\n",
    "            if fid_score is not None:\n",
    "                pbar_dict['FID'] = f'{fid_score:.2f}'\n",
    "            \n",
    "            pbar.set_postfix(pbar_dict)\n",
    "        \n",
    "        # Epoch summary\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        scheduler.step(avg_loss)\n",
    "        \n",
    "        summary = [f'Epoch {epoch+1}', f'Dim: {embedding_dim}', f'Loss: {avg_loss:.6f}']\n",
    "        \n",
    "        if consistency_loss_on:\n",
    "            summary.append(f'Consist: {epoch_consistency/len(train_loader):.6f}')\n",
    "        if target_loss_on:\n",
    "            summary.append(f'Target: {epoch_target/len(train_loader):.6f}')\n",
    "        if target_phase_loss_on:\n",
    "            summary.append(f'Phase: {epoch_target_phase/len(train_loader):.6f}')\n",
    "        if fid_score is not None:\n",
    "            summary.append(f'FID: {fid_score:.2f}')\n",
    "            if fid_score <= target_fid:\n",
    "                summary.append('TARGET!')\n",
    "        \n",
    "        print(' | '.join(summary))\n",
    "        \n",
    "        # Save checkpoint with dimension and eopch info\n",
    "        checkpoint_name = f'koopman_model_dim_{embedding_dim}_epoch_{epoch+1}.pth'\n",
    "        torch.save({\n",
    "            'model_state_dict': koopman_model.state_dict(),\n",
    "            'embedding_dim': embedding_dim,\n",
    "            'epoch': epoch + 1,\n",
    "            'fid_scores': fid_scores,\n",
    "            'target_fid': target_fid,\n",
    "            'config': {\n",
    "                'consistency_loss_on': consistency_loss_on,\n",
    "                'target_loss_on': target_loss_on,\n",
    "                'target_phase_loss_on': target_phase_loss_on,\n",
    "                'embedding_dim': embedding_dim,\n",
    "                'target_fid': target_fid\n",
    "            }\n",
    "        }, checkpoint_name)\n",
    "    \n",
    "    # Final FID summary, plot of FID vs epochs\n",
    "    if len(fid_scores) > 0:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(0, len(fid_scores)*5, 5), fid_scores, 'b-', linewidth=2, marker='o')\n",
    "        plt.axhline(y=target_fid, color='r', linestyle='--', label=f'Target FID ({target_fid:.2f})')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('FID Score')\n",
    "        plt.title(f'FID Evolution - Embedding Dim: {embedding_dim}')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'koopman_samples_dim_{embedding_dim}/fid_evolution.png', dpi=150)\n",
    "        plt.show()\n",
    "        \n",
    "        final_fid = fid_scores[-1]\n",
    "        best_fid = min(fid_scores)\n",
    "        \n",
    "        print(f\"Final FID: {final_fid:.2f}\")\n",
    "        print(f\"Best FID: {best_fid:.2f}\")\n",
    "        print(f\"Target FID: {target_fid:.2f}\")\n",
    "        print(f\"Embedding Dimension: {embedding_dim}\")\n",
    "    \n",
    "    return koopman_model\n",
    "\n",
    "def test_sampling(model_path=None, embedding_dim=None):\n",
    "    \"\"\"Test final sampling\"\"\"\n",
    "    \n",
    "    # If no model path provided, find the latest checkpoint\n",
    "    if model_path is None:\n",
    "        if embedding_dim is not None:\n",
    "            model_path = get_latest_checkpoint(embedding_dim)\n",
    "            if model_path is None:\n",
    "                print(f\"No checkpoints found for embedding_dim={embedding_dim}\")\n",
    "                return\n",
    "        else:\n",
    "            # Find any recent checkpoint\n",
    "            dim_checkpoints = glob.glob(\"koopman_model_dim_*_epoch_*.pth\")\n",
    "            if dim_checkpoints:\n",
    "                # Sort by modification time, get latest\n",
    "                dim_checkpoints.sort(key=os.path.getmtime, reverse=True)\n",
    "                model_path = dim_checkpoints[0]\n",
    "                print(f\"Using latest checkpoint: {model_path}\")\n",
    "            else:\n",
    "                print(\"No checkpoints with embedding dimension found\")\n",
    "                return\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Checkpoint not found: {model_path}\")\n",
    "        \n",
    "        # Suggest other checkpoints\n",
    "        print(\"Available checkpoints:\")\n",
    "        checkpoints = glob.glob(\"koopman_model_dim_*_epoch_*.pth\")\n",
    "        if checkpoints:\n",
    "            checkpoints.sort(key=os.path.getmtime, reverse=True)\n",
    "            for i, cp in enumerate(checkpoints[:5]):  # Show latest \n",
    "                print(f\"  {i+1}. {cp}\")\n",
    "        else:\n",
    "            print(\"  No checkpoints found\")\n",
    "            \n",
    "        return\n",
    "    \n",
    "    # Load checkpoint\n",
    "    try:\n",
    "        checkpoint = torch.load(model_path)\n",
    "        print(f\"Loading checkpoint: {model_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading checkpoint: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Get embedding dimension from checkpoint or parameter\n",
    "    if embedding_dim is None:\n",
    "        embedding_dim = checkpoint.get('embedding_dim', 1569)  # Defaults back to the setting without the projection matrix\n",
    "    \n",
    "    print(f\"Model configuration:\")\n",
    "    print(f\"  Embedding dimension: {embedding_dim}\")\n",
    "    if 'config' in checkpoint:\n",
    "        config = checkpoint['config']\n",
    "        print(f\"  Consistency Loss: {config.get('consistency_loss_on', 'Unknown')}\")\n",
    "        print(f\"  Target Loss: {config.get('target_loss_on', 'Unknown')}\")\n",
    "        print(f\"  Target Phase Loss: {config.get('target_phase_loss_on', 'Unknown')}\")\n",
    "        if 'target_fid' in config:\n",
    "            print(f\"  Target FID: {config['target_fid']:.2f}\")\n",
    "        print(f\"  Epoch: {checkpoint.get('epoch', 'Unknown')}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    koopman_model = KoopmanCFM(\n",
    "        num_channels=32,\n",
    "        num_res_blocks=1,\n",
    "        dim=(1, 28, 28),\n",
    "        embedding_dim=embedding_dim\n",
    "    ).to(device)\n",
    "    \n",
    "    # Load state dict\n",
    "    try:\n",
    "        koopman_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(\"Model loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model state: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Generate samples\n",
    "    print(\"Generating samples...\")\n",
    "    samples = sample_koopman(koopman_model, n_samples=64)\n",
    "    samples = samples * 0.5 + 0.5\n",
    "    grid = make_grid(samples, nrow=8, padding=2, normalize=False)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.imshow(grid.permute(1, 2, 0).cpu().numpy().squeeze(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Title with key info\n",
    "    title = f'Generated Samples - Embedding Dim: {embedding_dim}'\n",
    "    if 'config' in checkpoint and 'target_fid' in checkpoint['config']:\n",
    "        target_fid = checkpoint['config']['target_fid']\n",
    "        title += f'\\nTarget FID: {target_fid:.2f}'\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save with dimension in filename\n",
    "    save_path = f'final_koopman_samples_dim_{embedding_dim}.png'\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Samples saved to: {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "print(\"Koopman-CFM Training with Loss and Embedding Dimension Ablation\")\n",
    "    \n",
    "model = train_koopman_cfm(\n",
    "        # Loss ablation\n",
    "        consistency_loss_on=True,\n",
    "        target_loss_on=True,\n",
    "        target_phase_loss_on=True,\n",
    "        \n",
    "        # Dimension ablation  \n",
    "        embedding_dim=1765, \n",
    "        \n",
    "        # Training settings\n",
    "        n_epochs=2,  #Just to see if everything was working \n",
    "        batch_size=128,\n",
    "        lr=3e-4 #Learning rate used by Omri\n",
    "    )\n",
    "    \n",
    "# Test the trained model (will automatically find the right checkpoint)\n",
    "test_sampling(embedding_dim=1765)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchcfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
