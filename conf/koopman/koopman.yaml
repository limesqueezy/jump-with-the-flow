autoencoder:
  bottleneck:             false
  dim:                    ${wrapper.dim}
  num_channels:           ${wrapper.num_channels}
  num_res_blocks:         ${wrapper.num_res_blocks}
  channel_mult:           ${wrapper.channel_mult}
  num_heads:              ${wrapper.num_heads}
  num_head_channels:      ${wrapper.num_head_channels}
  attention_resolutions:  ${wrapper.attention_resolutions}
  dropout:                ${wrapper.dropout}
  learn_sigma:            ${wrapper.learn_sigma}
  class_cond:             ${wrapper.class_cond}
  use_checkpoint:         ${wrapper.use_checkpoint}
  use_fp16:               ${wrapper.use_fp16}
  use_new_attention_order: ${wrapper.use_new_attention_order}

operator:
  operator_dim:   null    # will be set in code:  raw_state_dim + latent_dim
  init_std:     0.001

consume:          # how many snapshots to feed the AE
  T: 100          # same as cfm.traj.traj_steps
  B: 6000         # same as cfm.traj.n_traj

train:
  # accelerator: gpu
  devices: [0]
  device: cuda

  # ─── Koopman DataModule settings ─────────────────────────────
  batch_size:     256
  num_workers:    4
  val_frac:       0.2
  t_grid:         ${koopman.consume.T}
  chunk_steps:    ${cfm.traj.chunk_size}
  # ───────────────────────────────────────────────────────────────

  max_epochs:     3

  autoencoder_lr: 0.001
  lie_lr:         0.0001
  lr_scheduler:   ReduceLROnPlateau
  weight_decay:   0.0 # This is only for the autoencoder, Operator is harcoded to 0

  gamma:          0.99
  delta_t:        0.01

  # Need those for LinearWarmupCosineAnnealingLR
  eta_min_frac:       0.2
  warmup_start_frac:  0.1

  grad_phase_weight_factor: 0.0

  decode_predict: true
  vae_loss:       false
  koop_reg:       false
  energy_penalty: false

  multistep:      false
  period:         10
  time_dep:       true
  num_iter:       100
  warmup_step:    10

  fid_interval:   500
  log_every_n_steps:                1
  check_val_every_n_train_steps:   200